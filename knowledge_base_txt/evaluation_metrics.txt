Lecture 20: Evaluating Machine Learning ModelsApplied Machine LearningVolodymyr KuleshovCornell T ech
Practical Considerations When Applying MachineLearningSuppose you trained an image classiﬁer with 80% accuracy. What's next?Add more data?Train the algorithm for longer?Use a bigger model?Add regularization?Add new features?We look at how to prioritize decisions to produce performant ML systems.
Part 1: Machine Learning Development WorkﬂowIn order to iterate and improve upon machine learning models, practitioners follow adevelopment workﬂow.We ﬁrst deﬁne it at a high-level. Afterwards, we will describe each step in more detail.
Review: Data DistributionIn machine learning, we typically assume that data comes from a probability distribution , which we will call the data distribution:The training set  consists of independent and identicalydistributed (IID) samples from ./uni2119/u1D465,/u1D466/uni223C/uni2119./uniE230={(,)/uni2223/u1D456=1,2,...,/u1D45B}/u1D465(/u1D456)/u1D466(/u1D456)/uni2119
Review: Hold-Out SetA hold-out set  consists of independent and identicalydistributed (IID) samples from  and is distinct from the training set.A model that generalizes is accurate on a hold-out set.={(,)/uni2223/u1D456=1,2,...,/u1D45B}/uniE230˙/u1D465(/u1D456)˙/u1D466(/u1D456)˙/uni2119We present a workﬂow for developing accurate models that generalize.
Datasets for Model DevelopmentWhen developing machine learning models, it is customary to work with three datasets:Training set: Data on which we train our algorithms.Development set (validation or hold-out set): Data used for tuning algorithms.T est set: Data used to evaluate the ﬁnal performance of the model.
Model Development WorkﬂowThe typical way in which these datasets are used is:1. Training: Try a new model and ﬁt it on the training set.1. Model Selection: Estimate performance on the development set using metrics.Based on results, try a new model idea in step #1.1. Evaluation: Finally, estimate real-world performance on test set.
A few extra notes about this procedure:ML development is a trial-and-error process. You need to try a lot of ideas!When starting on a new problem, a ﬁrst step is to establish train, dev, and test setsand performance metrics.This workﬂow allows you to iterate very quickly.
Development and Test SetsDev Set: Its goal is to detect changes in model performance.T est Set: Its goal is conﬁdently estimate real-world performanceChoosing the best model on the dev set over-estimates its peformance. Hence, aclean and untouched test set is crucial!
Choosing Dev and Test SetsHow should one choose the development and test set? We highlight two considerations.
Distributional Consistency: The development and test sets should be from the datadistribution we will see in production.This is because we want to be optimizing performance in deployment (e.g.,classiﬁcation accuracy on dog images).The training data could potentially be from a different distribution (e.g., includeother types of animal images).
Dataset Size: Dev and test datasets need to estimate future performance.The classic advice is to save 30\% of data for dev+test.In big data world, 30\% can be too much. Using e.g. 3,000 inputs for testing out of1M is enough.
Model SelectionHere, we again highlight two considerations.Choosing Metrics: The model development workﬂow requires optimizing a singleperformance metric.If multiple metrics are important, they can be combined into one (e.g., precision +recall  F-score).Other metrics (e.g., computational performance) can be incorporated asconstraints./uni2192
Updating the Model: We select hyperparameters based on dev set performance and:Inuition, e.g., if the model overﬁts, regularize it more.Hyperparameter grid search or random search.Sometimes, we use specialized algorithms, e.g. Bayesian hyperparameteroptimization.We will provide much more detail on the intuition part later!
Example: Training a Neural NetConsider a workﬂow for building a neural image classiﬁer.1. We start with a standard CNN that gets 90\% dev set accuracy.1. We tune dropout via grid search on dev set; accuracy is 95\% now.1. We try a new idea -- we add residual connections to the CNN and retrain it. Thisbrings dev set accuracy to 99\%!1. We are happy with this performance. We measure test set accuracy: 97\%, stillquite good!
Limitations of the ML WorkﬂowYou may encounter a number of issues:1. Overﬁtting dev set after repeatedly choosing the best model on it.2. Dev and test sets may no longer represent true data distribution.3. The metric may no longer measure true performance.In such cases you need to collect more data and/or change the metric.
Part 2: Evaluating Classiﬁcation ModelsThe ﬁrst step towards building better ML models is to determine how to evaluate them.We will start by talking about how to evaluate classiﬁcation models.
Review: ClassiﬁcationConsider a training dataset .We distinguish between two types of supervised learning problems depnding on thetargets .1. Regression: The target variable  is continuous: .2. Classiﬁcation: The target variable  is discrete, and takes on one of  possiblevalues.When classiﬁcation labels take  values, we perform binary classiﬁcation./uniE230={(,),(,),…,(,)}/u1D465(1)/u1D466(1)/u1D465(2)/u1D466(2)/u1D465(/u1D45B)/u1D466(/u1D45B)/u1D466(/u1D456)/u1D466/uni2208/uniE245/uniE245/uni2286/uni211D/u1D466/u1D43E/u1D43E=2
An example of a classiﬁcation task is the Iris ﬂower dataset.In [1]:# import standard machine learning librariesimport numpy as npimport pandas as pdfrom sklearn import datasetsfrom sklearn.model_selection import train_test_splitiris = datasets.load_iris() # load the Iris datasetX, y = iris.data[:120, :2], iris.target[:120] # create imbalanced classes and only use first 2 featuresX, X_holdout, y, y_holdout = train_test_split(X, y, test_size=50, random_state=0)
We may visualize this dataset in 2D.In [2]:from matplotlib import pyplot as pltplt.rcParams['figure.figsize'] = [12, 4]# Visualize the Iris flower datasetsetosa_flowers = (iris.target == 0)plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.Paired)plt.ylabel("Sepal width (cm)")plt.xlabel("Sepal length (cm)")Out[2]:
Text(0.5, 0, 'Sepal length (cm)')
Review: Machine Learning ModelsA machine learning model is a functionthat maps inputs  to targets ./u1D453:/uniE244/uni2192/uniE245/u1D465/uni2208/uniE244/u1D466/uni2208/uniE245
Below, we ﬁt a Softmax model to the Iris dataset.In [3]:from sklearn.linear_model import LogisticRegression# fit a softmax regression model (implemented in LogisticRegression in sklearn)model = LogisticRegression()model.fit(X,y)y_pred = model.predict(X_holdout)
Classiﬁcation AccuracyThe simplest and most natural metric for classiﬁcation algorithms is accuracy:where  is an indicator function (equals 1 if its input is true and zero otherwise).acc(/u1D453)=/u1D540{/u1D453(/u1D465)=/u1D466},1/u1D45B/uni2211/u1D456=1/u1D45B/u1D540{/uni22C5}
In [4]:accuracy = (y_pred == y_holdout).mean()print('Iris holdout set accuracy: %.2f' % accuracy)Iris holdout set accuracy: 0.84
Confusion MatrixWe can better understand classiﬁcation error via a confusion matrix.On the y-axis we have the true classes .On the x-axis we have the predicted classes .The cell for  contains the number of samples of class  that have beenclassiﬁed as ./u1D466/u1D457/u1D466/uni0302/u1D458(,)/u1D466/u1D457/u1D466/uni0302/u1D458/u1D466/u1D457/u1D466/uni0302/u1D458
In [5]:from sklearn.metrics import plot_confusion_matrixplot_confusion_matrix(model, X_holdout, y_holdout)Out[5]:
<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x12241b4e0>
Metrics for Binary ClassiﬁcationWe can look at performance in a more precise way when we do binary classiﬁcation.Consider the following confusion matrix.Predicted positive Predicted negative Positive class True positive (TP)False negative (FN)Negative class False positive (FP)True negative (TN)=1/u1D466/u1D466/uni0302=0/u1D466/u1D466/uni0302/u1D466=1/u1D466=0
We can deﬁne accuracy as follows:This is the number of correction predictions divided by the total number of predictions.accuracy=TP+TNTP+FP+FN+TN
Sensitivity and SpeciﬁcityPredicted positive Predicted negative Positive class True positive (TP)False negative (FN)Negative class False positive (FP)True negative (TN)We can also look at "accuracy" on each class separately. This reveals problems withimbalanced classes.=1/u1D466/u1D466/uni0302=0/u1D466/u1D466/uni0302/u1D466=1/u1D466=0sensitivity(recall, true positive rate)speciﬁcity(true negative rate)==TPpositive classTPTP+FN==TNnegative classTNTN+FP
We can combine these into a single measure called balanced accuracybalanced accuracy=(speciﬁcity+sensitivity)12=(+)12TNTN+FPTPTP+FN
Precision and RecallPredicted positive Predicted negative Positive class True positive (TP)False negative (FN)Negative class False positive (FP)True negative (TN)An alternative set of measures is precision and recall.=1/u1D466/u1D466/uni0302=0/u1D466/u1D466/uni0302/u1D466=1/u1D466=0precision(positive predictive value)recall(sensitivity, true positive rate)==TPpredicted positiveTPTP+FP==TPpositive classTPTP+FN
Imagine we are building a search engine. The positive examples are the pages that arerelevant to the users.Precision: of the web pages returned by the engine, how many were truly relevant?Recall: how many relevant pages did we ﬁnd?Notice that we don't directly report performance on negatives (what % of irrelevant pageswere labeled as such).
Whe do we choose precision and recall vs. sensitivity and speciﬁcity?Precision and recall are useful if we don't care about true negatives (e.g. a searchengine). We only care about identifying correct positives and not missing any.Sensitivity and speciﬁcity are useful if negatives are also important (e.g., accuratelydetecting absence of cancer is also important).
F-ScoreThe F-Score is the geometric mean of precision and recall.It equals one at prefect prediction and recall and zero if one of precision or recall is zero.F-Score=2+1precision1recall
Part 3: Advanced Classiﬁcation MetricsNext, we look a few more advanced classiﬁcation metrics.
Review: ClassiﬁcationConsider a training dataset .We distinguish between two types of supervised learning problems depnding on thetargets .1. Regression: The target variable  is continuous: .2. Classiﬁcation: The target variable  is discrete, and takes on one of  possiblevalues.When classiﬁcation labels take  values, we perform binary classiﬁcation./uniE230={(,),(,),…,(,)}/u1D465(1)/u1D466(1)/u1D465(2)/u1D466(2)/u1D465(/u1D45B)/u1D466(/u1D45B)/u1D466(/u1D456)/u1D466/uni2208/uniE245/uniE245/uni2286/uni211D/u1D466/u1D43E/u1D43E=2
Review: Sensitivity and SpeciﬁcityPredicted positive Predicted negative Positive class True positive (TP)False negative (FN)Negative class False positive (FP)True negative (TN)We can also look at "accuracy" on each class separately. This reveals problems withimbalanced classes.=1/u1D466/u1D466/uni0302=0/u1D466/u1D466/uni0302/u1D466=1/u1D466=0sensitivity(recall, true positive rate)speciﬁcity(true negative rate)==TPpositive classTPTP+FN==TNnegative classTNTN+FP
Trading O! Sensitivity and SpeciﬁcitySuppose that true positives are more important than true negatives.We may output the positive class only on very conﬁdent examples.This will increase performance on positive examples and decrease performance onnegative examples.Most classiﬁers come with conﬁdence scores that make this easy to do.
With our softmax model, we can simpliy obtain the class probabilities.The default threshold for predicting class 1 in binary classiﬁcation is when it has >50\%probability. But we can set it higher or lower.In [7]:pred_probabilities = model.predict_proba(X_holdout)print('Predicted probabilities of class 0 from the model:')print(pred_probabilities[:10,0])Predicted probabilities of class 0 from the model:[0.90500627 0.1628024  0.26248388 0.91899931 0.05452794 0.98241773 0.90444375 0.87543531 0.83226506 0.12415388]
Receiver Operating Characteristic (ROC)In binary classiﬁcation, the Receiver Operating Characteristic (ROC) curve plots the truepositive rate and the false positive rate (FPR) as we vary the threshold for labeling apositive example.TPR=true positive rate(recall, sensitivity)FPR=1−speciﬁcity(true negative rate)==TPpositive classTPTP+FN=1−=TNnegative classFPTP+FP
Suppose we want to improve sensitivity for Class 2 on the Iris dataset. We ﬁrst computethe probability  for each input . For any threshold , we label  as Class 2if .Small  result in a high TPR (we identify all positives) and high FPR (many are false).High  result in a low TPR (we identify few positives) and low FPR (we only label themost conﬁdent inputs)./u1D45D(/u1D466=2/uni007C/u1D465)/u1D465/u1D461>0/u1D465/u1D45D(/u1D466=2/uni007C/u1D465)>/u1D461/u1D461/u1D461
Bewlow, the ROC curve measures the TPR and FPR as we vary ./u1D461In [8]:from sklearn.metrics import roc_curveclass2_scores = pred_probabilities[:,2] # we take class 2 as the "positive" class# create labels where class 2 is the "positive" classclass2_y = np.zeros(y_holdout.shape)class2_y[y_holdout==2] = 1print('First class 2 scores: ', class2_scores[:4])fpr, tpr, thresholds = roc_curve(class2_y, class2_scores)First class 2 scores:  [0.02495207 0.15064683 0.17470687 0.00545101]
We can visualize the TPR vs. the FPR at various thresholds.
In [9]:plt.figure(figsize=(6,6))plt.plot(fpr, tpr, color='darkorange')plt.plot([0, 1], [0, 1], color='navy', linestyle='--')plt.xlim([0.0, 1.0])plt.ylim([0.0, 1.05])plt.xlabel('False Positive Rate')plt.ylabel('True Positive Rate')plt.title('Receiver operating characteristic')Out[9]:
Text(0.5, 1.0, 'Receiver operating characteristic')

We highlight the following properties of the ROC curve:In the bottom left corner, we predict only negatives. TPR = FPR = 0.In the top right corner, we predict only positives. TPR = FPR = 1.The ideal classiﬁer is in the top left: TPR = 1, FPR = 0.The blue diagonal corresponds to randomly guessing "positive" with .The ROC curve lies between ideal and random./u1D45D=TPR
Area Under the CurveWe can use the area under the curve (AUC) as a single measure of classiﬁer performance.The ideal classiﬁer (ROC curve reaches top left corner) has an AUC-ROC of one.A random classiﬁer (diagonal dashed line) has an AUC-ROC of 0.5.
We may compute the AUC of the above ROC curve as follows.In [10]:from sklearn.metrics import aucprint('AUC-ROC: %.4f' % auc(fpr, tpr))AUC-ROC: 0.8555
Multi-Class GeneralizationsWe can also deﬁne sensitivity, speciﬁcity, and other metrics for each class in multi-classclassiﬁcation.We deﬁne each class in turn to be "positive" () and all other classes asnegative ().We compute sensitivity and speciﬁcity for each "positive" class using the usualformulas./u1D466=1/u1D466=0
In multi-class settings, we average binary metrics in various ways:macro: We average binary one-vs-all metrics for each class.micro: We average binary metrics for each point.See the scikit-learn  for more on model evaluation.=precisionmacro1/u1D43E/uni2211/u1D458=1/u1D43ETP/u1D458+TP/u1D458FP/u1D458=precisionmicro/uni2211/u1D43E/u1D458=1TP/u1D458(+)/uni2211/u1D43E/u1D458=1TP/u1D458FP/u1D458guide (https://scikit-learn.org/stable/modules/model_evaluation.html)
In [6]:from sklearn.metrics import classification_reportprint(classification_report(y_holdout, y_pred, target_names=['Setosa', 'Versicolor', 'Virginica']))              precision    recall  f1-score   support      Setosa       0.95      1.00      0.97        19  Versicolor       0.79      0.92      0.85        24   Virginica       0.50      0.14      0.22         7    accuracy                           0.84        50   macro avg       0.75      0.69      0.68        50weighted avg       0.81      0.84      0.81        50
Part 4: Evaluating Regression ModelsThe ﬁrst step towards building better ML models is to determine how to evaluate them.Next, we look at regression models.
Review: RegressionConsider a training dataset .We distinguish between two types of supervised learning problems depnding on thetargets .1. Regression: The target variable  is continuous: .2. Classiﬁcation: The target variable  is discrete, and takes on one of  possiblevalues./uniE230={(,),(,),…,(,)}/u1D465(1)/u1D466(1)/u1D465(2)/u1D466(2)/u1D465(/u1D45B)/u1D466(/u1D45B)/u1D466(/u1D456)/u1D466/uni2208/uniE245/uniE245/uni2286/uni211D/u1D466/u1D43E
Review: Machine Learning ModelsA machine learning model is a functionthat maps inputs  to targets ./u1D453:/uniE244/uni2192/uniE245/u1D465/uni2208/uniE244/u1D466/uni2208/uniE245
Regression LossesMost standard regression losses can be used as evaluation metrics.Mean squared error:Absolute (L1) error:12/u1D45B/uni2211/u1D456=1/u1D45B(/u1D453()−)/u1D465(/u1D456)/u1D466(/u1D456)2/u1D453()−1/u1D45B/uni2211/u1D456=1/u1D45B/uni2223/uni2223/u1D465(/u1D456)/u1D466(/u1D456)/uni2223/uni2223
In [11]:from sklearn.metrics import mean_squared_error, mean_absolute_errory1 = np.array([1, 2, 3, 4])y2 = np.array([-1, 1, 3, 5])print('Mean squared error: %.2f' % mean_squared_error(y1, y2))print('Mean absolute error: %.2f' % mean_absolute_error(y1, y2))Mean squared error: 1.50Mean absolute error: 1.00
These metrics have a number of limitations:They do not take the scale of the input into account. The same magnitude of theerror has different implications depending on whether  or .They are not easily interpretable. It's unclear if an L1 error of 10 is very good orvery bad.=100/u1D466(/u1D456)=10/u1D466(/u1D456)
Scaled LossesT o account for differences in scale, we may work with scaled losses:Mean absolute percent error (MAPE):Symmetric mean absolute percent error (SMAPE):The SMAPE allows either  or  to be small (or zero).1/u1D45B/uni2211/u1D456=1/u1D45B/u1D453()−/uni2223/uni2223/u1D465(/u1D456)/u1D466(/u1D456)/uni2223/uni2223/uni007C/uni007C/u1D466(/u1D456)1/u1D45B/uni2211/u1D456=1/u1D45B/u1D453()−/uni2223/uni2223/u1D465(/u1D456)/u1D466(/u1D456)/uni2223/uni2223(/uni007C/uni007C+/uni007C/u1D453()/uni007C)/2/u1D466(/u1D456)/u1D465(/u1D456)/u1D466(/u1D456)/u1D453()/u1D465(/u1D456)
Scaled Logarithmic LossesAnother way to account for error in  relative to  is by taking the log of bothvalues. This puts them on the same scale.This is called the mean absolute logarithmic error (MALE)./u1D453()/u1D465(/u1D456)/u1D466(/u1D456)log(1+)−log(1+/u1D453())1/u1D45B/uni2211/u1D456=1/u1D45B/uni2223/uni2223/u1D466(/u1D456)/u1D465(/u1D456)/uni2223/uni2223
The Coe"cient of DeterminationThe coefﬁcient of determination, usually denoted by , measures how accuracy of thepredictions, relative to constantly predicting the average :An  of one corresponds to perfect accuracy. An  of zero means that  is not betterthan the average prediction./u1D4452=/u1D466¯1/u1D45B/uni2211/u1D45B/u1D456=1/u1D466(/u1D456)=1−()./u1D4452/uni2211/u1D45B/u1D456=1(/u1D453()−)/u1D465(/u1D456)/u1D466(/u1D456)2/uni2211/u1D45B/u1D456=1(−)/u1D466¯/u1D466(/u1D456)2/u1D4452/u1D4452/u1D453In [ ]: 
