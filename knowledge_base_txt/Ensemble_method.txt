Ensem ble Metho ds in Mac hine LearningThomas G/. Dietteric hOregon State Univ ersit y /, Corv allis/, Oregon/, USA/,tgd/@cs/.orst/.edu /,WWW home page/: http/:////www/.cs/.orst/.edu///~tgdAbstract/. Ensem ble metho ds are learning algorithms that construct aset of classi/ers and then classify new data p oin ts b y taking a /(w eigh ted/)v ote of their predictions/. The original ensem ble metho d is Ba y esian a v er/-aging/, but more recen t algorithms include error/-correcting output co ding/,Bagging/, and b o osting/. This pap er reviews these metho ds and explains
wh ye n s e m bles can often p erform b etter than an y single classi/er/. Someprevious studies comparing ensem ble metho ds are review ed/, and somenew exp erimen ts are presen ted to unco v er the reasons that Adab o ostdo es not o v er/t rapidly /./1 In tro ductionConsider the standard sup ervised learning problem/. A learning program is giv entraining examples of the form f /( x/1
/;;y/1
/) /;;/:/:/:/;; /( xm
/;;ym
/) g for some unkno wn func/-tion y /= f /( x /)/. The xi
v alues are t ypically v ectors of the form h xi/;; /1
/;;xi/;; /2
/;;/:/:/:/;;xi/;;n
iwhose comp onen ts are discrete/- or real/-v alued suc h as heigh t/, w eigh t/, color/, age/,and so on/. These are also called the fe atur es of xi
/. Let us use the notation xijto refer to the j /-th feature of xi
/. In some situations/, w e will drop the i subscriptwhen it is implied b y the con text/.The y v alues are t ypically dra wn from a discrete set of classes f /1 /;;/:/:/: /;;K gin the case of classi/c ation or from the real line in the case of r e gr ession /. Inthis c hapter/, w e will consider only classi/cation/. The training examples ma yb ecorrupted b y some random noise/.Giv en a set S of training examples/, a learning algorithm outputs a classi/er /.The classi/er is an h yp othesis ab out the true function f /.G i v en new x v alues/, itpredicts the corresp onding y v alues/. I will denote classi/ers b y h/1
/;;/:/:/:/;;hL
/.An ensem ble of classi/ers is a set of classi/ers whose individual decisions arecom bined in some w a y/( t ypically b yw eigh t e do ru n w eigh ted v oting/) to classifynew examples/. One of the most activ e areas of researc h in sup ervised learning hasb een to study metho ds for constructing go o d ensem bles of classi/ers/. The maindisco v ery is that ensem bles are often m uc h more accurate than the individualclassi/ers that mak et h e m u p /.A necessary and su/cien t condition for an ensem ble of classi/ers to b e moreaccurate than an y of its individual mem b ers is if the classi/ers are accurate anddiv erse /(Hansen /& Salamon/, /1/9/9/0/)/. An accurate classi/er is one that has anerror rate of b etter than random guessing on new x v alues/. Tw o classi/ers are
/2div erse if they mak e di/eren t errors on n e wd a t ap o i n ts/. T o see wh y accuracyand div ersit y are g ood /, imagine that w e ha v e an ensem ble of three classi/ers/:f h/1
/;;h/2
/;;h/3
g and consider a new case x /. If the three classi/ers are iden tical /(i/.e/./,not div erse/)/, then when h/1
/( x /) is wrong/, h/2
/( x /) and h/3
/( x /) will also be wrong/.Ho w ev er/, if the errors made b y the classi/ers are uncorrelated/, then when h/1
/( x /)is wrong/, h/2
/( x /)a n d h/3
/( x /)m a y b e correct/, so that a ma jorit yv ote will correctlyclassify x /. More precisely /, if the error rates of L h yp otheses h/`
are all equal top/< /1 /= /2 and if the errors are indep enden t/, then the probabilit y that the ma jorit yv ote will b e wrong will b e the area under the binomial distribution where morethan L/= /2h yp otheses are wrong/. Figure /1 sho ws this for a sim ulated ensem bleof /2/1 h yp otheses/, eac hh a ving an error rate of /0/./3/. The area under the curv ef o r/1/1 or more h yp otheses b eing sim ultaneously wrong is /0/./0/2/6/, whic hi s m uc h lessthan the error rate of the individual h yp otheses/.
00.020.040.060.080.10.120.140.160.180.2
0 5 10 15 20Probability
Number of classifiers in errorFig/. /1/. The probabilit y that exactly /` /(of /2/1/) h yp otheses will mak e an error/, assumingeac hh yp othesis has an error rate of /0/./3 and mak es its errors indep enden tly of the otherh yp otheses/.Of course/, if the individual h yp otheses mak e uncorrelated errors at rates ex/-ceeding /0/./5/, then the error rate of the v oted ensem ble will incr e ase as a result ofthe v oting/. Hence/, one k ey to successful ensem ble metho ds is to construct indi/-vidual classi/ers with error rates b elo w /0/./5 whose errors are at least somewhatuncorrelated/.This formal c haracterization of the problem is in triguing/, but it do es notaddress the question of whether it is p ossible in practice to construct go o d en/-
sem bles/. F ortunately /, it is often p ossible to construct v ery go o d ensem bles/. Thereare three fundamen tal reasons for this/.
/3The /rst reason is statistical/. A learning algorithm can b e view ed as searc h/-ing a space H of h yp otheses to iden tify the b est h yp othesis in the space/. Thestatistical problem arises when the amoun t of training data a v ailable is to o smallcompared to the size of the h yp othesis space/. Without su/cien t data/, the learn/-ing algorithm can /nd man y di/eren t h yp otheses in H that all giv e the sameaccuracy on the training data/. By constructing an ensem ble out of all of theseaccurate classi/ers/, the algorithm can /\a v erage/" their v otes and reduce the riskof c ho osing the wrong classi/er/. Figure /2/(top left/) depicts this situation/. Theouter curv e denotes the h yp othesis space H /. The inner curv e denotes the set ofh yp otheses that all giv e go o d accuracy on the training data/. The p oin t lab eled fis the true h yp othesis/, and w e can see that b ya v eraging the accurate h yp otheses/,w e can /nd a go o d appro ximation to f /.
H H
HStatistical Computational
Representationalh1
h3 h4h2
ff
fh1
h2h3
h1
h2
h3Fig/. /2/. Three fundamen tal reasons wh ya n ensem ble ma yw o r kb e t t e rt h a na singleclassi/er
/4The second reason is computational/. Man y learning algorithms w ork b y p er/-forming some form of lo cal searc ht h a t m a yg e ts t u c k in lo cal optima/. F or ex/-ample/, neural net w ork algorithms emplo y gradien t descen t to minimize an errorfunction o v er the training data/, and decision tree algorithms emplo y a greedysplitting rule to gro w the decision tree/. In cases where there is enough trainingdata /(so that the statistical problem is absen t/)/, it ma y still be v ery di/cultcomputationally for the learning algorithm to /nd the b est h yp othesis/. Indeed/,optimal training of b oth neural net w orks and decisions trees is NP/-hard /(Hy a/l/&R i v est/, /1/9/7/6/;; Blum /& Riv est/, /1/9/8/8/)/. An ensem ble constructed b y running thelo cal searc h from man y di/eren t starting p oin ts ma yp r o vide a b etter appro xi/-mation to the true unkno wn function than an yo ft h e individual classi/ers/, assho wn in Figure /2 /(top righ t/)/.The third reason is represen tational/. In most applications of mac hine learn/-ing/, the true function f cannot b e represen ted b ya n yo f t h e h yp otheses in H /.By forming w eigh ted sums of h yp otheses dra wn from H /, it ma y be p ossibleto expand the space of represen table functions/. Figure /2 /(b ottom/) depicts thissituation/.The represen tational issue is somewhat subtle/, b ecause there are man y learn/-ing algorithms for whic h H is/, in principle/, the space of all p ossible classi/ers/. F orexample/, neural net w orks and decision trees are bo t h v ery /exible algorithms/.Giv en enough training data/, they will explore the space of all p ossible classi/ers/,and sev eral p eople ha v e pro v ed asymptotic represen tation theorems for them/(Hornik/, Stinc hcom b e/, /& White/, /1/9/9/0/)/. Nonetheless/, with a /nite training sam/-ple/, these algorithms will explore only a /nite set of h yp otheses and they willstop searc hing when they /nd an h yp othesis that /ts the training data/. Hence/,in Figure /2/, w em ust consider the space H to b e the e/ectiv e space of h yp othesessearc hed b y the learning algorithm for a giv en training data set/.These three fundamen tal issues are the three most imp ortan tw a ys in whic hexisting learning algorithms fail/. Hence/, ensem ble metho ds ha v e the promise ofreducing /(and p erhaps ev en eliminating/) these three k ey shortcomings of stan/-dard learning algorithms/./2 Metho ds for Constructing Ensem blesMan y metho ds for constructing ensem bles ha v e b een dev elop ed/. Here w e willreview general purp ose metho ds that can b e applied to man yd i / e r e n t learningalgorithms/./2/./1 Ba y esian V oting/: En umerating the Hyp othesesIn a Ba y esian probabilistic setting/, eac hh yp othesis h de/nes a conditional prob/-abilit y distribution/: h /( x /)/= P /( f /( x /)/= y j x /;;h /)/. Giv en a new data p oin t x and atraining sample S /, the problem of predicting the v alue of f /( x /) can be view edas the problem of computing P /( f /( x /)/= y j S/;; x /)/. W e can rewrite this as w eigh ted
/5sum o v er all h yp otheses in H /:P /( f /( x /)/= y j S/;; x /)/=
Xh /2H
h /( x /) P /( h j S /) /:W e can view this as an ensem ble metho d in whic h the ensem ble consists of all ofthe h yp otheses in H /, eac hw eigh ted b y its p osterior probabilit y P /( h j S /)/. By Ba y esrule/, the p osterior probabilit y is prop ortional to the lik e l i h ood of the trainingdata times the prior probabilit yo f h /:P /( h j S /) // P /( S j h /) P /( h /) /:In some learning problems/, it is p ossible to completely en umerate eac h h /2H /,compute P /( S j h /) and P /( h /)/, and /(after normalization/)/, ev aluate this Ba y esian/\committee/./" F urthermore/, if the true function f is dra wn from H according toP /( h /)/, then the Ba y esian v oting sc heme is optimal/.Ba y esian v oting primarily addresses the statistical comp onen t of ensem/-bles/. When the training sample is small/, man y h yp otheses h will ha v e signif/-ican tly large p osterior probabilities/, and the v oting pro cess can a v erage these to/\marginalize a w a y/" the remaining uncertain t y ab out f /. When the training sam/-ple is large/, t ypically only one h yp othesis has substan tial p osterior probabilit y /,and the /\ensem ble/" e/ectiv ely shrinks to con tain only a single h yp othesis/.In complex problems where H cannot b e en umerated/, it is sometimes p ossibleto appro ximate Ba y esian v oting b y dra wing a random sample of h yp othesesdistributed according to P /( h j S /)/. Recen t w ork on Mark o v c hain Mon te Carlometho ds /(Neal/, /1/9/9/3/) seeks to dev elop a set of to ols for this task/.The most idealized asp ect of the Ba y esian analysis is the prior b elief P /( h /)/. Ifthis prior completely captures all of the kno wledge that w eh a v e ab out f b eforew e obtain S /,t h e n b y de/nition w e cannot do b etter/. But in practice/, it is oftendi/cult to construct a space H and assign a prior P /( h /) that captures our priorkno wledge adequately /. Indeed/, often H and P /( h /)a r e c hosen for computationalcon v enience/, and they are kno wn to b e inadequate/. In suc h cases/, the Ba y esiancommittee is not optimal/, and other ensem ble metho ds ma y pro duce be t t e rresults/. In particular/, the Ba y esian approac h do es not address the computationaland represen tational problems in an y signi/can tw a y /./2/./2 Manipulating the T raining ExamplesThe second metho d for constructing ensem bles manipulates the training exam/-ples to generate m ultiple h yp otheses/. The learning algorithm is run sev eral times/,eac h time with a di/eren t subset of the training examples/. This tec hnique w orksesp ecially w ell for unstable learning algorithms/|algorithms whose output clas/-si/er undergo es ma jor c hanges in resp onse to small c hanges in the training data/.Decision/-tree/, neural net w ork/, and rule learning algorithms are all unstable/. Lin/-ear regression/, nearest neigh b or/, and linear threshold algorithms are generallyv ery stable/.
/6The most straigh tforw ard w a y of manipulating the training set is called Bag/-ging /.O ne a c h run/, Bagging presen ts the learning algorithm with a training setthat consists of a sample of m training examples dra wn randomly with replace/-men t from the original training set of m items/. Suc h a training set is called ab o otstr ap r eplic ate of the original training set/, and the tec hnique is called b o ot/-str ap aggr e gation /(from whic h the term Bagging is deriv ed/;; Breiman/, /1/9/9/6/)/. Eac hb o otstrap replicate con tains/, on the a v erage/, /6/3/./2/% of the original training set/,with sev eral training examples app earing m ultiple times/.Another training set sampling metho d is to construct the training sets b ylea ving out disjoin t subsets of the training data/. F or example/, the training setcan b e randomly divided in to /1/0 disjoin t subsets/. Then /1/0 o v erlapping trainingsets can be constructed b y dropping out a di/eren t one of these /1/0 subsets/.This same pro cedure is emplo y ed to construct training sets for /1/0/-fold cross/-v alidation/, so ensem bles constructed in this w a y are sometimes called cr oss/-validate dc ommitte es /(P arman to/, Munro/, /& Do yle/, /1/9/9/6/)/.The third metho d for manipulating the training set is illustrated b y theAd aBoost algorithm/, dev elop ed b y F reund and Sc hapire /(/1/9/9/5/, /1/9/9/6/, /1/9/9/7/,/1/9/9/8/)/. Lik e Bagging/, Ad aBoost manipulates the training examples to generatem ultiple h yp otheses/. Ad aBoost main tains a set of w eigh ts o v er the trainingexamples/. In eac h iteration /` /, the learning algorithm is in v ok ed to minimizethe w eigh ted error on the training set/, and it returns an h yp othesis h/`
/. Thew eigh ted error of h/`
is computed and applied to up date the w eigh ts on thetraining examples/. The e/ect of the c hange in w eigh ts is to place more w eigh ton training examples that w ere misclassi/ed b y h/`
and less w eigh t on examplesthat w ere correctly classi/ed/. In subsequen t iterations/, therefore/, Ad aBoostconstructs progressiv ely more di/cult learning problems/.The /nal classi/er/, hf
/( x /)/=
P/`
w/`
h/`
/( x /)/, is constructed b ya w eigh ted v oteof the individual classi/ers/. Eac h classi/er is w eigh ted /(b y w/`
/) according to itsaccuracy on the w eigh ted training set that it w as trained on/.Recen t researc h /(Sc hapire /& Singer/, /1/9/9/8/) has sho wn that Ad aBoost can b eview ed as a stage/-wise algorithm for minimizing a particular error function/. T ode/ne this error function/, supp ose that eac h training example is lab eled as /+/1or /; /1/, corresp onding to the p ositiv e and negativ e examples/. Then the quan tit ymi
/= yi
h /( xi
/) is p ositiv ei f h correctly classi/es xi
and negativ e otherwise/. Thisquan tit y mi
is called the mar gin of classi/er h on the training data/. Ad aBoostcan b e seen as trying to minimizeXi
exp
/ /; yi
X/`
w/`
h/`
/( xi
/)
/!/;; /(/1/)whic h is the negativ e exp onen tial of the margin of the w eigh ted v oted classi/er/.This can also b e view ed as attempting to maximize the margin on the trainingdata/.
/7/2/./3 Manipulating the Input F eaturesA third general tec hnique for generating m ultiple classi/ers is to manipulatethe set of input fe atur es a v ailable to the learning algorithm/. F or example/, in apro ject to iden tify v olcano es on V en us/, Cherk auer /(/1/9/9/6/) trained an ensem bleof /3/2 neural net w orks/. The /3/2 net w orks w ere based on /8 di/eren t subsets ofthe /1/1/9 a v ailable input features and /4 di/eren tn e t w ork sizes/. The input featuresubsets w ere selected /(b y hand/) to group together features that w ere based ondi/eren t image pro cessing op erations /(suc h as principal comp onen t analysis andthe fast fourier transform/)/. The resulting ensem ble classi/er w as able to matc hthe p erformance of h uman exp erts in iden tifying v olcano es/. T umer and Ghosh/(/1/9/9/6/) applied a similar tec hnique to a sonar dataset with /2/5 input features/.Ho w ev er/, they found that deleting ev en a few of the input features h urt thep erformance of the individual classi/ers so m uc h that the v oted ensem ble didnot p erform v ery w ell/. Ob viously /, this tec hnique only w orks when the inputfeatures are highly redundan t/./2/./4 Manipulating the Output T argetsA fourth general tec hnique for constructing a go o d ensem ble of classi/ers is tomanipulate the y v alues that are giv en to the learning algorithm/. Dietteric h/&Bakiri /(/1/9/9/5/) describ e a tec hnique called error/-correcting output co ding/. Supp osethat the n um be r of classes/, K /, is large/. Then new learning problems can beconstructed b y randomly partioning the K classes in to t w o subsets A/`
and B/`
/.The input data can then b e re/-lab eled so that an y of the original classes in setA/`
are giv en the deriv ed lab el /0 and the original classes in set B/`
are giv enthe deriv ed lab el /1/. This relab eled data is then giv en to the learning algorithm/,whic h constructs a classi/er h/`
/. By rep eating this pro cess L times /(generatingdi/eren t subsets A/`
and B/`
/)/, w e obtain an ensem ble of L classi/ers h/1
/;;/:/:/:/;;hL
/.No wg i v en a new data p oin t x /,h o w should w e classify it/? The answ er is toha v e eac h h/`
classify x /. If h/`
/( x /) /=/0 /, then eac h class in A/`
receiv es a v ote/. Ifh/`
/( x /) /= /1/, then eac h class in B/`
receiv es a v ote/. After eac ho ft h e L classi/ershas v oted/, the class with the highest n um be r o f v otes is selected as the predictionof the ensem ble/.An equiv alen t w a y of thinking ab out this metho d is that eac h class j isenco ded as an L /-bit co dew ord Cj
/, where bit /` is /1 if and only if j /2 B/`
/. The/` /-th learned classi/er attempts to predict bit /` of these co dew ords/. When the Lclassi/ers are applied to classify a new p oin t x /, their predictions are com binedin to an L /-bit string/. W e then c ho ose the class j whose co dew ord Cj
is closest /(inHamming distance/) to the L /-bit output string/. Metho ds for designing go o d error/-correcting co des can be applied to c ho ose the co dew ords Cj
/(or equiv alen tly /,subsets A/`
and B/`
/)/.Dietteric h and Bakiri rep ort that this tec hnique impro v es the p erformance ofb oth the C/4/./5 decision tree algorithm and the bac kpropagation neural net w orkalgorithm on a v ariet y of di/cult classi/cation problems/. Recen tly /, Sc hapire
/8/(/1/9/9/7/) has sho wn ho w Ad aBoost can b e com bined with error/-correcting out/-put co ding to yield an excellen te n s e m ble classi/cation metho d that he calls Ad/-aBoost/.OC /. The p erformance of the metho d is sup erior to the ECOC metho d/(and to Bagging/)/, but essen tially the same as another /(quite complex/) algorithm/,called Ad aBoost/.M/2 /. Hence/, the main adv an tage of Ad aBoost/.OC is imple/-men tation simplicit y/: It can w ork with an y learning algorithm for solving /2/-classproblems/.Ricci and Aha /(/1/9/9/7/) applied a metho d that com bines error/-correcting out/-put co ding with feature selection/. When learning eac h classi/er/, h/`
/,t h e ya p p l yfeature selection tec hniques to c ho ose the b est features for learning that classi/er/.They obtained impro v emen ts in /7 out of /1/0 tasks with this approac h/./2/./5 Injecting RandomnessThe last general purp ose metho d for generating ensem bles of classi/ers is toinject randomness in to the learning algorithm/. In the bac kpropagation algorithmfor training neural net w orks/, the initial w eigh ts of the net w ork are set randomly /.If the algorithm is applied to the same training examples but with di/eren tinitial w eigh ts/, the resulting classi/er can be quite di/eren t /(Kolen /&P ollac k/,/1/9/9/1/)/.While this is p erhaps the most common w a y of generating ensem bles of neu/-ral net w orks/, manipulating the training set ma yb e more e/ectiv e/. A study b yP arman to/, Munro/, and Do yle /(/1/9/9/6/) compared this tec hnique to Bagging and to/1/0/-fold cross/-v alidated committees/. They found that cross/-v alidated committeesw ork ed b est/, Bagging second b est/, and m ultiple random initial w eigh ts thirdb est on one syn thetic data set and t w o medical diagnosis data sets/.F or the C/4/./5 decision tree algorithm/, it is also easy to inject randomness/(Kw ok /& Carter/, /1/9/9/0/;; Dietteric h/, /2/0/0/0/)/. The k ey decision of C/4/./5 is to c ho ose afeature to test at eac hi n ternal no de in the decision tree/. A t eac hi n ternal no de/,C/4/./5 applies a criterion kno wn as the information gain ratio to rank/-order thev arious p ossible feature tests/. It then c ho oses the top/-rank ed feature/-v alue test/.F or discrete/-v alued features with V v alues/, the decision tree splits the data in toV subsets/, dep ending on the v alue of the c hosen feature/. F or real/-v alued features/,the decision tree splits the data in to /2 subsets/, dep ending on whether the v alueof the c hosen feature is ab o v e or be l o w a c hosen threshold/. Dietteric h /(/2/0/0/0/)implemen ted av arian t of C/4/./5 that c ho oses randomly /(with equal probabilit y/)among the top /2/0 b est tests/. Figure /3 compares the p erformance of a singlerun of C/4/./5 to ensem bles of /2/0/0 classi/ers o v er /3/3 di/eren t data sets/. F or eac hdata set/, a p oin t is plotted/. If that p oin t lies b elo w the diagonal line/, then theensem ble has lo w er error rate than C/4/./5/. W e can see that nearly all of the p oin tslie be l o w the line/. A statistical analysis sho ws that the randomized trees dostatistically signi/can tly b etter than a single decision tree on /1/4 of the data setsand statistically the same in the remaining /1/9 data sets/.Ali /& P azzani /(/1/9/9/6/) injected randomness in to the F OIL algorithm for learn/-ing Prolog/-st yle rules/. F OIL w orks somewhat lik e C/4/./5 in that it ranks p ossibleconditions to add to a rule using an information/-gain criterion/. Ali and P azzani
/9
0102030405060
0 10 20 30 40 50 60200-fold Randomized C4.5 (percent error)
C4.5 (percent error)Fig/. /3/. Comparison of the error rate of C/4/./5 to an ensem ble of /2/0/0 decision treesconstructed b y injecting randomness in to C/4/./5 and then taking a uniform v ote/.computed all candidate conditions that scored within /8/0/% of the top/-rank ed can/-didate/, and then applied a w eigh ted random c hoice algorithm to c ho ose amongthem/. They compared ensem bles of /1/1 classi/ers to a single run of F OIL andfound statistically signi/can ti m p r o v emen ts in /1/5 out of /2/9 tasks and statisticallysigni/can t loss of p erformance in only one task/. They obtained similar resultsusing /1/1/-fold cross/-v alidation to construct the training sets/.Ra viv and In trator /(/1/9/9/6/) com bine b o otstrap sampling of the training datawith injecting noise in to the input features for the learning algorithm/. T ot r a i neac hm e m be r of an ensem ble of neural net w orks/, they dra w training exampleswith replacemen t from the original training data/. The x v alues of eac h trainingexample are p erturb ed b y adding Gaussian noise to the input features/. Theyrep ort large impro v emen ts in a syn thetic b enc hmark task and a medical diagnosistask/.Finally /, note that Mark o vc hain Mon te Carlo metho ds for constructing Ba y esianensem bles also w ork b y injecting randomness in to the learning pro cess/. Ho w ev er/,instead of taking a uniform v ote/, as w e did with the randomized decision trees/,eac hh yp othesis receiv es a v ote prop ortional to its p osterior probabilit y /./3 Comparing Di/eren t Ensem ble Metho dsSev eral exp erimen tal studies ha v e b een p erformed to compare ensem ble metho ds/.The largest of these are the studies b y Bauer and Koha vi /(/1/9/9/9/) and b y Dietteric h/(/2/0/0/0/)/. T able /1 summarizes the results of Dietteric h/'s study /. The table sho wsthat Ad aBoost often giv es the b est results/. Bagging and randomized trees giv e
/1/0similar p erformance/, although randomization is able to do b etter in some casesthan Bagging on v ery large data sets/.T able /1/. All pairwise com binations of the four ensem ble metho ds/. Eac hc e l lc o n tainsthe n um b er of wins/, losses/, and ties b et w een the algorithm in that ro w and the algorithmin that column/.C/4/./5 Ad aBoost C/4/./5 Bagged C/4/./5
Random C/4/./5
 /1 /4/{/0/{/1 /9
 /1/{/7/{/2 /5
 /6/{/3/{/2 /4
Bagged C/4/./5
 /1 /1/{/0/{/2 /2
 /1/{/8/{/2 /4
Ad aBoost C/4/./5
 /1 /7/{/0/{/1 /6
Most of the data sets in this study had little or no noise/. When /2/0/% arti/cialclassi/cation noise w as added to the /9 domains where Bagging and Ad aBoostga v e di/eren t p erformance/, the results shifted radically as sho wn in T able /2/.Under these conditions/, Ad aBoost o v er/ts the data badly while Bagging issho wn to w ork v ery w ell in the presence of noise/. Randomized trees did not dov ery w ell/.T able /2/. All pairwise com binations of C/4/./5/, Ad aBoost ed C/4/./5/, Bagged C/4/./5/, andRandomized C/4/./5 on /9 domains with /2/0/% syn thetic class lab el noise/. Eac hc e l lc o n tainsthe n um b er of wins/, losses/, and ties b et w een the algorithm in that ro w and the algorithmin that column/.C/4/./5 Ad aBoost C/4/./5 Bagged C/4/./5
Random C/4/./5
 /5/{/2/{/2
 /5/{/0/{/4
 /0/{/2/{/7
Bagged C/4/./5
 /7/{/0/{/2
 /6/{/0/{/3
Ad aBoost C/4/./5
 /3/{/6/{/0
The k ey to understanding these results is to return again to the three short/-comings of existing learning algorithms/: statistical supp ort/, computation/, andrepresen tation/. F or the decision/-tree algorithm C/4/./5/, all three of these prob/-lems can arise/. Decision trees essen tially partition the input feature space in torectangular regions whose sides are p erp endicular to the co ordinate axes/. Eac hrectangular region corresp onds to one leaf no de of the tree/.If the true function f can be represen ted b y a small decision tree/, thenC/4/./5 will w ork w ell without an y ensem ble/. If the true function can b e correctlyrepresen ted b y a large decision tree/, then C/4/./5 will need a v ery large trainingdata set in order to /nd a go o d /t/, and the statistical problem will arise/.The computational problem arises b ecause /nding the b est /(i/.e/./, smallest/)decision tree consisten t with the training data is computationally in tractable/, soC/4/./5 mak es a series of decisions greedily /. If one of these decisions is made incor/-rectly /, then the training data will b e incorrectly partitioned/, and all subsequen tdecisions are lik ely to be a/ected/. Hence/, C/4/./5 is highly unstable/, and small
/1/1c hanges in the training set can pro duce large c hanges in the resulting decisiontree/.The represen tational problem arises b ecause of the use of rectangular parti/-tions of the input space/. If the true decision b oundaries are not orthogonal tothe co ordinate axes/, then C/4/./5 requires a tree of in/nite size to represen t thoseb oundaries correctly /.I n terestingly /,a v oted com bination of small decision treesis equiv alen tt oam uc h larger single tree/, and hence/, an ensem ble metho d canconstruct a go o d appro ximation to a diagonal decision b oundary using sev eralsmall trees/. Figure /4 sho ws an example of this/. On the left side of the /gureare plotted three decision b oundaries constructed b y three decision trees/, eac hof whic h uses /5 in ternal no des/. On the righ ti st h e b oundary that results froma simple ma jorit y v ote of these trees/. It is equiv alen t to a single tree with /1/3in ternal no des/, and it is m uc h more accurate than an y one of the three individualtrees/.
Class 1
Class 2Class 1
Class 2Fig/. /4/. The left /gure sho ws the true diagonal decision b oundary and three staircaseappro ximations to it /(of the kind that are created b y decision tree algorithms/)/. Therigh t /gure sho ws the v oted decision b oundary /,w h i c hi sam uc h b etter appro ximationto the diagonal b oundary /.No w let us consider the three algorithms/: Ad aBoost /, Bagging/, and Ran/-domized trees/. Bagging and Randomization b oth construct eac h decision treeindep enden tly of the others/. Bagging accomplishes this b y manipulating the in/-put data/, and Randomization directly alters the c hoices of C/4/./5/. These metho dsare acting somewhat lik eB a y esian v oting/;; they are sampling from the space ofall p ossible h yp otheses with a bias to w ard h yp otheses that giv e go o d accuracyon the training data/. Consequen tly /, their main e/ect will b e to address the sta/-tistical problem and/, to a lesser exten t/, the computational problem/. But they donot directly attempt to o v ercome the represen tational problem/.In con trast/, Ad aBoost constructs eac h new decision tree to eliminate /\resid/-ual/" errors that ha v e not b een prop erly handled b y the w eigh ted v ote of thepreviously/-constructed trees/. Ad aBoost is directly trying to optimize the w eigh tedv ote/. Hence/, it is making a direct assault on the represen tational problem/. Di/-
/1/2rectly optimizing an ensem ble can increase the risk of o v er/tting/, b ecause thespace of ensem bles is usually m uc h larger than the h yp othesis space of the orig/-inal algorithm/.This explanation is consisten t with the exp erimen tal results giv en ab o v e/. Inlo w/-noise cases/, Ad aBoost giv es go o d p erformance/, b ecause it is able to opti/-mize the ensem ble without o v er/tting/. Ho w ev er/, in high/-noise cases/, Ad aBoostputs a large amoun to fw eigh t on the mislab eled examples/, and this leads it too v er/t v ery badly /. Bagging and Randomization do w ell in bo t h the noisy andnoise/-free cases/, b ecause they are fo cusing on the statistical problem/, and noiseincreases this statistical problem/.Finally /,w e can understand that in v ery large datasets/, Randomization canb e exp ected to do b etter than Bagging b ecause b o otstrap replicates of a largetraining set are v ery similar to the training set itself/, and hence/, the learneddecision tree will not b e v ery div erse/. Randomization creates div ersit y under allconditions/, but at the risk of generating lo w/-qualit y decision trees/.Despite the plausibilit y of this explanation/, there is still one imp ortan to p e nquestion concerning Ad aBoost /. Giv en that Ad aBoost aggressiv ely attemptsto maximize the margins on the training set/, wh y do esn/'t it o v er/t more often/?P art of the explanation ma y lie in the /\stage/-wise/" nature of Ad aBoost /. Ineac h iteration/, it rew eigh ts the training examples/, constructs a new h yp othesis/,and c ho oses a w eigh t w/`
for that h yp othesis/. It nev er /\bac ks up/" and mo di/esthe previous c hoices of h yp otheses or w eigh ts that it has made to comp ensatefor this new h yp othesis/.T o test this explanation/, I conducted a series of simple exp erimen ts on syn/-thetic data/. Let the true classi/er f b e a simple decision rule that tests just onefeature /(feature /0/) and assigns the example to class /+/1 if the feature is /1/, andto class /; /1 if the feature is /0/. No w construct training /(and testing/) examples b ygenerating feature v ectors of length /1/0/0 at random as follo ws/. Generate feature/0 /(the imp ortan t feature/) at random/. Then generate eac h of the other featuresrandomly to agree with feature /0 with probabilit y /0/./8 and to disagree otherwise/.Assign lab els to eac h training example according to the true function f /, butwith /1/0/% random classi/cation noise/. This creates a di/cult learning problemfor simple decision rules of this kind /(decision stumps/)/, b ecause all /1/0/0 features
are correlated with the class/. Still/, a large ensem ble should b e able to do w ell onthis problem b yv oting separate decision stumps for eac h feature/.I constructed a v ersion of Ad aBoost that w orks more aggressiv ely than stan/-dard Ad aBoost /. After ev ery new h yp othesis h/`
is constructed and its w eigh tassigned/, m yv ersion p erforms a gradien t descen ts e a r c h to minimize the negativ eexp onen tial margin /(equation /1/)/. Hence/, this algorithm reconsiders the w eigh tsof all of the learned h yp otheses after eac h new h yp othesis is added/. Then itrew eigh ts the training examples to re/ect the revised h yp othesis w eigh ts/.Figure /5 sho ws the results when training on a training set of size /2/0/. The plotcon/rms our explanation/. The Aggressiv e Ad aBoost initially has m uc h highererror rates on the test set than Standard Ad aBoost /. It then gradually im/-pro v es/. Mean while/, Standard Ad aBoost initially obtains excellen t p erformance
/1/3on the test set/, but then it o v er/ts as more and more classi/ers are added to theensem ble/. In the limit/, bo t h ensem bles should ha v e the same represen tationalprop erties/, be c a u s e they are b oth minimizing the same function /(equation /1/)/.But w e can see that the exceptionally go o d p erformance of Standard Ad aBooston this problem is due to the stage/-wise optimization pro cess/, whic hi s s l o wt o/t the data/.
160165170175180185190195200205210
1 10 100 1000Errors (out of 1000) on the test data set
Iterations of AdaboostStandard AdaboostAggressive AdaboostFig/. /5/. Aggressiv e Ad aBoost exhibits m uc hw orse p erformance than Standard Ad/-aBoost o nac hallenging syn thetic problem/4 ConclusionsEnsem bles are w ell/-established as a metho d for obtaining highly accurate classi/-/ers b yc o m bining less accurate ones/. This pap er has pro vided a brief surv ey ofmetho ds for constructing ensem bles and review ed the three fundamen tal reasonswh y ensem ble metho ds are able to out/-p erform an y single classi/er within theensem ble/. The pap er has also pro vided some exp erimen tal results to elucidateone of the reasons wh y Ad aBoost p erforms so w ell/.One op en question not discussed in this pap er concerns the in teraction b e/-t w een Ad aBoost and the prop erties of the underlying learning algorithm/. Mostof the learning algorithms that ha v e b een com bined with Ad aBoost ha v e b eenalgorithms of a global c haracter /(i/.e/./, algorithms that learn a relativ ely lo w/-dimensional decision b oundary/)/. It w ould be in teresting to see whether lo calalgorithms /(suc h as radial basis functions and nearest neigh b or metho ds/) can b epro/tably com bined via Ad aBoost to yield in teresting new learning algorithms/.
Bibliograph yA l i /,K /.M /. /,/& P azzani/, M/. J/. /(/1/9/9/6/)/. Error reduction through learning m ultipledescriptions/. Machine L e arning /, /2/4 /(/3/)/, /1/7/3/{/2/0/2/.Bauer/, E/./, /& Koha vi/, R/. /(/1/9/9/9/)/. An empirical comparison of v oting classi/cationalgorithms/: Bagging/, b o osting/, and v arian ts/. Machine L e arning /, /3/6 /(/1///2/)/,/1/0/5/{/1/3/9/.Blum/, A/./, /& Riv est/, R/. L/. /(/1/9/8/8/)/. T raining a /3/-no de neural net w ork is NP/-Complete /(Extended abstract/)/. In Pr o c e e dings of the /1/9/8/8 Workshop onComputational L e arning The ory /, pp/. /9/{/1/8 San F rancisco/, CA/. MorganKaufmann/.Breiman/, L/. /(/1/9/9/6/)/. Bagging predictors/. Machine L e arning /, /2/4 /(/2/)/, /1/2/3/{/1/4/0/.Cherk auer/, K/. J/. /(/1/9/9/6/)/. Human exp ert/-lev el p erformance on a scien ti/cimage analysis task b y a system using com bined arti/cial neural net/-w orks/. In Chan/, P /. /(Ed/./)/, Working Notes of the AAAI Workshopon Inte gr ating Multiple L e arne d Mo dels /, pp/. /1/5/{/2/1/. Av ailable fromhttp/:////www/.cs/.f it/. ed u// /~im lm // /.Dietteric h/, T/. G/. /(/2/0/0/0/)/. An exp erimen tal comparison of three metho ds forconstructing ensem bles of decision trees/: Bagging/, b o osting/, and random/-ization/. Machine L e arning /.Dietteric h/, T/. G/./, /& Bakiri/, G/. /(/1/9/9/5/)/. Solving m ulticlass learning problems viaerror/-correcting output co des/. Journal of A rti/cial Intel ligenc e R ese ar ch /,/2 /, /2/6/3/{/2/8/6/.F reund/, Y/./, /& Sc hapire/, R/. E/. /(/1/9/9/5/)/. A decision/-theoretic generalization ofon/-line learning and an application to b o osting/. T ec h/. rep/./, A T/&T BellLab oratories/, Murra y Hill/, NJ/.F reund/, Y/./, /& Sc hapire/, R/. E/. /(/1/9/9/6/)/. Exp erimen ts with a new b o osting algo/-rithm/. In Pr o c/. /1/3th International Confer enc e on Machine L e arning /,p p /./1/4/8/{/1/4/6/. Morgan Kaufmann/.Hansen/, L/./, /& Salamon/, P /. /(/1/9/9/0/)/. Neural net w ork ensem bles/. IEEE T r ans/.Pattern A nalysis and Machine Intel l/. /, /1/2 /, /9/9/3/{/1/0/0/1/.Hornik/, K/./, Stinc hcom be /, M/./, /& White/, H/. /(/1/9/9/0/)/. Univ ersal appro ximationof an unkno wn mapping and its deriv ativ es using m ultila y er feedforw ardnet w orks/. Neur al Networks /, /3 /, /5/5/1/{/5/6/0/.Hy a/l/, L/./, /& Riv est/, R/. L/. /(/1/9/7/6/)/. Constructing optimal binary decision trees isNP/-Complete/. Information Pr o c essing L etters /, /5 /(/1/)/, /1/5/{/1/7/.Kolen/, J/. F/./, /& P ollac k/, J/. B/. /(/1/9/9/1/)/. Bac k propagation is sensitiv e to initialconditions/. In A dvanc es in Neur al Information Pr o c essing Systems /,V ol/. /3/,pp/. /8/6/0/{/8/6/7 San F rancisco/, CA/. Morgan Kaufmann/.Kw ok/, S/. W/./, /& Carter/, C/. /(/1/9/9/0/)/. Multiple decision trees/. In Sc hac h ter/, R/. D/./,Levitt/, T/. S/./, Kannal/, L/. N/./, /& Lemmer/, J/. F/. /(Eds/./)/, Unc ertainty in A r/-ti/cial Intel ligenc e/4 /, pp/. /3/2/7/{/3/3/5/. Elsevier Science/, Amsterdam/.
/1/5Neal/, R/. /(/1/9/9/3/)/. Probabilistic inference using Mark o vc hain Mon te Carlo meth/-od s /. T ec h/. rep/. CR G/-TR/-/9/3/-/1/, Departmen t of Computer Science/, Univ er/-sit yo fT oron to/, T oron to/, CA/.P arman to/, B/./, Munro/, P /. W/./, /& Do yle/, H/. R/. /(/1/9/9/6/)/. Impro ving committeediagnosis with resampling tec hniques/. In T ouretzky /, D/. S/./, Mozer/, M/. C/./,/& Hesselmo/, M/. E/. /(Eds/./)/, A dvanc es in Neur al Information Pr o c essingSystems /,V ol/. /8/, pp/. /8/8/2/{/8/8/8 Cam bridge/, MA/. MIT Press/.Ra viv/, Y/./, /& In trator/, N/. /(/1/9/9/6/)/. Bo otstrapping with noise/: An e/ectiv er e g u /-larization tec hnique/. Conne ction Scienc e /, /8 /(/3/{/4/)/, /3/5/5/{/3/7/2/.Ricci/, F/./, /& Aha/, D/. W/. /(/1/9/9/7/)/. Extending lo cal learners with error/-correctingoutput co des/. T ec h/. rep/./, Na v al Cen ter for Applied Researc h in Arti/cialIn telligence/, W ashington/, D/.C/.Sc hapire/, R/. E/. /(/1/9/9/7/)/. Using output co des to b o ost m ulticlass learning prob/-lems/. In Pr o c e e dings of the F ourte enth International Confer enc e on Ma/-chine L e arning /, pp/. /3/1/3/{/3/2/1 San F rancisco/, CA/. Morgan Kaufmann/.Sc hapire/, R/. E/./, F reund/, Y/./, Bartlett/, P /./, /& Lee/, W/. S/. /(/1/9/9/7/)/. Bo osting the mar/-gin/: A new explanation for the e/ectiv e n e s so fv oting metho ds/. In Fisher/,D/. /(Ed/./)/, Machine L e arning/: Pr o c e e dings of the F ourte enth InternationalConfer enc e /. Morgan Kaufmann/.Sc hapire/, R/. E/./, /& Singer/, Y/. /(/1/9/9/8/)/. Impro v ed b o osting algorithms usingcon/dence/-rated predictions/. In Pr o c/. /1/1th A nnu/. Conf/. on Comput/. L e arn/-ing The ory /, pp/. /8/0/{/9/1/. A CM Press/, New Y ork/, NY/.T umer/, K/./, /& Ghosh/, J/. /(/1/9/9/6/)/. Error correlation and error reduction in ensem bleclassi/ers/. Conne ction Scienc e /, /8 /(/3/{/4/)/, /3/8/5/{/4/0/4/.
