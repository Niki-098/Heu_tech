Contents
3 Data Preprocessing 3
3.1 Data Preprocessing: An Overview . . . . . . . . . . . . . . . . . 4
3.1.1 Data Quality: Why Preprocess the Data? . . . . . . . . . 4
3.1.2 Major Tasks in Data Preprocessing . . . . . . . . . . . . . 5
3.2 Data Cleaning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3.2.1 Missing Values . . . . . . . . . . . . . . . . . . . . . . . . 8
3.2.2 Noisy Data . . . . . . . . . . . . . . . . . . . . . . . . . . 9
3.2.3 Data Cleaning as a Process . . . . . . . . . . . . . . . . . 12
3.3 Data Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
3.3.1 The Entity Identiﬁcation Problem . . . . . . . . . . . . . 14
3.3.2 Redundancy and Correlation Analysis . . . . . . . . . . . 15
3.3.3 Tuple Duplication . . . . . . . . . . . . . . . . . . . . . . 19
3.3.4 Detection and Resolution of Data Value Conﬂicts . . . . . 19
3.4 Data Reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
3.4.1 Overview of Data Reduction Strategies . . . . . . . . . . 20
3.4.2 Wavelet Transforms . . . . . . . . . . . . . . . . . . . . . 21
3.4.3 Principal Components Analysis . . . . . . . . . . . . . . . 23
3.4.4 Attribute Subset Selection . . . . . . . . . . . . . . . . . . 24
3.4.5 Regression and Log-Linear Models: Parametric Data Re -
duction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
3.4.6 Histograms . . . . . . . . . . . . . . . . . . . . . . . . . . 27
3.4.7 Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
3.4.8 Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
3.4.9 Data Cube Aggregation . . . . . . . . . . . . . . . . . . . 31
3.5 Data Transformation and Data Discretization . . . . . . . . . . . 32
3.5.1 Overview of Data Transformation Strategies . . . . . . . . 32
3.5.2 Data Transformation by Normalization . . . . . . . . . . 34
3.5.3 Discretization by Binning . . . . . . . . . . . . . . . . . . 36
3.5.4 Discretization by Histogram Analysis . . . . . . . . . . . . 3 7
3.5.5 Discretization by Cluster, Decision Tree, and Correl ation
Analyses . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
3.5.6 Concept Hierarchy Generation for Nominal Data . . . . . 3 8
3.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
1
2 CONTENTS
3.8 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . 45
Chapter 3
Data Preprocessing
Today’s real-world databases are highly susceptible to noi sy, missing, and inconsistent data
due to their typically huge size (often several gigabytes or more) and their
likely origin from multiple, heterogenous sources. Low-qu ality data will lead
to low-quality mining results. “How can the data be preprocessed in order to
help improve the quality of the data and, consequently, of th e mining results?
How can the data be preprocessed so as to improve the eﬃciency and ease of the
mining process?”
There are a number of data preprocessing techniques. Data cleaning can be
applied to remove noise and correct inconsistencies in the d ata.Data integration
merges data from multiple sources into a coherent data store , such as a data
warehouse. Data reduction can reduce the data size by aggregating, eliminating
redundant features, or clustering, for instance. Data transformations , such as
normalization, may be applied, where data are scaled to fall within a smaller
range like 0.0 to 1.0. This can improve the accuracy and eﬃcie ncy of mining
algorithms involving distance measurements. These techni ques are not mutu-
ally exclusive; they may work together. For example, data cl eaning can involve
transformations to correct wrong data, such as by transform ing all entries for
adateﬁeld to a common format. In Chapter 2, we learned about the diﬀ erent
attribute types and how to use basic statistical descriptio ns to study charac-
teristics of the data. These can help identify erroneous val ues and outliers,
which will be useful in the data cleaning and integration ste ps. Data processing
techniques, when applied before mining, can substantially improve the overall
quality of the patterns mined and/or the time required for th e actual mining.
In this chapter, we introduce the basic concepts of data prep rocessing in
Section 3.1. The methods for data preprocessing are organiz ed into the following
categories: data cleaning (Section 3.2), data integration (Section 3.3), data
reduction (Section 3.4), and data transformation (Section 3.5).
3
4 CHAPTER 3. DATA PREPROCESSING
3.1 Data Preprocessing: An Overview
This section presents an overview of data preprocessing. Se ction 3.1.1 illustrates
the many elements deﬁning data quality. This provides the in centive behind
data preprocessing. Section 3.1.2 outlines the major tasks in data preprocessing.
3.1.1 Data Quality: Why Preprocess the Data?
Data has quality if it satisﬁes the requirements of its inten ded use. There are
many factors comprising data quality . These include: accuracy, completeness,
consistency, timeliness, believability , andinterpretability .
Imagine that you are a manager at AllElectronics and have been charged
with analyzing the company’s data with respect to the sales a t your branch. You
immediately set out to perform this task. You carefully insp ect the company’s
database and data warehouse, identifying and selecting the attributes or dimen-
sions to be included in your analysis, such as item,price, andunitssold. Alas!
You notice that several of the attributes for various tuples have no recorded
value. For your analysis, you would like to include informat ion as to whether
each item purchased was advertised as on sale, yet you discov er that this in-
formation has not been recorded. Furthermore, users of your database system
have reported errors, unusual values, and inconsistencies in the data recorded for
some transactions. In other words, the data you wish to analy ze by data mining
techniques are incomplete (lacking attribute values or certain attributes of inter-
est, or containing only aggregate data), inaccurate ornoisy (containing errors,
or values that deviate from the expected), and inconsistent (e.g., containing
discrepancies in the department codes used to categorize it ems). Welcome to
the real world!
This scenario illustrates three of the elements deﬁning dat a quality - accu-
racy,completeness , andconsistency . Inaccurate, incomplete, and inconsis-
tent data are commonplace properties of large real-world da tabases and data
warehouses. There are many possible reasons for inaccurate data (having in-
correct attribute values). The data collection instrument s used may be faulty.
There may have been human or computer errors occurring at dat a entry. Users
may purposely submit incorrect data values for mandatory ﬁe lds when they do
not wish to submit personal information, e.g., by choosing t he default value
‘January 1’ displayed for birthday. (This is known as disguised missing data .)
Errors in data transmission can also occur. There may be tech nology limita-
tions, such as limited buﬀer size for coordinating synchron ized data transfer
and consumption. Incorrect data may also result from incons istencies in nam-
ing conventions or data codes used, or inconsistent formats for input ﬁelds, such
asdate. Duplicate tuples also require data cleaning.
Incomplete data can occur for a number of reasons. Attribute s of interest
may not always be available, such as customer information fo r sales transaction
data. Other data may not be included simply because they were not considered
important at the time of entry. Relevant data may not be recor ded due to
a misunderstanding, or because of equipment malfunctions. Data that were
3.1. DATA PREPROCESSING: AN OVERVIEW 5
inconsistent with other recorded data may have been deleted . Furthermore, the
recording of the history or modiﬁcations to the data may have been overlooked.
Missing data, particularly for tuples with missing values f or some attributes,
may need to be inferred.
Recall that data quality depends on the intended use of the da ta. Two
diﬀerent users may have very diﬀerent assessments of the qua lity of a given
database. For example, a marketing analyst may need to acces s the database
mentioned above for a list of customer addresses. Some of the addresses are
outdated or incorrect, yet overall, 80% of the addresses are accurate. The
marketing analyst considers this to be a large customer data base for target
marketing purposes and is pleased with the accuracy of the da tabase, although,
as sales manager, you found the data inaccurate.
Timeliness also aﬀects data quality. Suppose that you are overseeing th e
distribution of monthly sales bonuses to the top sales repre sentatives at AllElec-
tronics . Several sales representatives, however, fail to submit th eir sales records
on time at the end of the month. There are also a number of corre ctions and
adjustments that ﬂow in after the month’s end. For a period of time following
each month, the data stored in the database is incomplete. Ho wever, once all
of the data is received, it is correct. The fact that the month -end data is not
updated in a timely fashion has a negative impact on the data q uality.
Two other factors aﬀecting data quality are believability a nd interpretabil-
ity.Believability reﬂects how much the data are trusted by users, while inter-
pretability reﬂects how easy the data are understood. Suppose that a data base,
at one point, had several errors, all of which have since been corrected. The past
errors, however, had caused many problems for users in the sa les department,
and so they no longer trust the data. The data also use many acc ounting codes,
which the sales department does not know how to interpret. Ev en though such
a database is now accurate, complete, consistent, and timel y, users from the
sales department may regard it as of low quality due to poor be lievability and
interpretability.
3.1.2 Major Tasks in Data Preprocessing
In this section, we look at the major steps involved in data pr eprocessing,
namely, data cleaning, data integration, data reduction, a nd data transforma-
tion.
Data cleaning routines work to “clean” the data by ﬁlling in missing values ,
smoothing noisy data, identifying or removing outliers, an d resolving inconsis-
tencies. If users believe the data are dirty, they are unlike ly to trust the results
of any data mining that has been applied to it. Furthermore, d irty data can
cause confusion for the mining procedure, resulting in unre liable output. Al-
though most mining routines have some procedures for dealin g with incomplete
or noisy data, they are not always robust. Instead, they may c oncentrate on
avoiding overﬁtting the data to the function being modeled. Therefore, a useful
preprocessing step is to run your data through some data clea ning routines.
Section 3.2 discusses methods for cleaning up your data.
6 CHAPTER 3. DATA PREPROCESSING
Getting back to your task at AllElectronics , suppose that you would like
to include data from multiple sources in your analysis. This would involve
integrating multiple databases, data cubes, or ﬁles, that i s,data integration .
Yet some attributes representing a given concept may have di ﬀerent names in
diﬀerent databases, causing inconsistencies and redundan cies. For example, the
attribute for customer identiﬁcation may be referred to as customer idin one
data store and custidin another. Naming inconsistencies may also occur for
attribute values. For example, the same ﬁrst name could be re gistered as “Bill”
in one database, but “William” in another, and “B.” in the thi rd. Furthermore,
you suspect that some attributes may be inferred from others (e.g., annual
revenue). Having a large amount of redundant data may slow do wn or confuse
the knowledge discovery process. Clearly, in addition to da ta cleaning, steps
must be taken to help avoid redundancies during data integra tion. Typically,
data cleaning and data integration are performed as a prepro cessing step when
preparing the data for a data warehouse. Additional data cle aning can be
performed to detect and remove redundancies that may have re sulted from data
integration.
“Hmmm,” you wonder, as you consider your data even further. “The data
set I have selected for analysis is HUGE, which is sure to slow down the mining
process. Is there a way I can reduce the size of my data set with out jeopardizing
the data mining results?” Data reduction obtains a reduced representation
of the data set that is much smaller in volume, yet produces th e same (or
almost the same) analytical results. Data reduction strate gies include dimen-
sionality reduction andnumerosity reduction . Indimensionality reduction ,
data encoding schemes are applied so as to obtain a reduced or “compressed”
representation of the original data. Examples include data compression tech-
niques (such as wavelet transforms andprincipal components analysis ) as well
asattribute subset selection (e.g., removing irrelevant attributes), and attribute
construction (e.g., where a small set of more useful attributes is derived from
the original set). In numerosity reduction , the data are replaced by alter-
native, smaller representations using parametric models ( such as regression or
log-linear models ) or nonparametric models (such as with histograms, clusters ,
sampling , ordata aggregation ). Data reduction is the topic of Section 3.4.
Getting back to your data, you have decided, say, that you wou ld like to
use a distance-based mining algorithm for your analysis, su ch as neural net-
works, nearest-neighbor classiﬁers, or clustering.1Such methods provide better
results if the data to be analyzed have been normalized , that is, scaled to a
smaller range such as [0.0, 1.0]. Your customer data, for exa mple, contain the
attributes ageandannual salary . The annual salary attribute usually takes
much larger values than age. Therefore, if the attributes are left unnormal-
ized, the distance measurements taken on annual salary will generally outweigh
distance measurements taken on age.Discretization andconcept hierarchy gen-
eration can also be useful, where raw data values for attributes are r eplaced
1Neural networks and nearest-neighbor classiﬁers are descr ibed in Chapter 8, and clustering
is discussed in Chapters 10 and 11.
3.1. DATA PREPROCESSING: AN OVERVIEW 7
Data cleaning
Data integration
Data transformation
Data reduction attributes attributes
A1 A2 A3 ... A12622, 32, 100, 59, 48 20.02, 0.32, 1.00, 0.59, 0.48
T1
T2
T3
T4
...
T2000transactions
transactionsA1 A3 ...
T1
T4
...
T1456A115
Figure 3.1: Forms of data preprocessing. NOTE to editor: Fig ure needs to be
redone so that data reduction comes before data transformat ion. Thanks.
by ranges or higher conceptual levels. For example, raw valu es for agemay be
replaced by higher-level concepts, such as youth,adult, orsenior . Discretization
and concept hierarchy generation are powerful tools for dat a mining in that they
allow the mining of data at multiple levels of abstraction. N ormalization, data
discretization, and concept hierarchy generation are form s ofdata transfor-
mation . You soon realize such data transformation operations are a dditional
data preprocessing procedures that would contribute towar d the success of the
mining process. Data integration and data discretization a re discussed in
Sections 3.5.
Figure 3.1 summarizes the data preprocessing steps describ ed here. Note
that the above categorization is not mutually exclusive. Fo r example, the re-
moval of redundant data may be seen as a form of data cleaning, as well as data
reduction.
In summary, real-world data tend to be dirty, incomplete, an d inconsistent.
Datapreprocessingtechniquescanimprovethequalityofth edata,therebyhelping
toimprovetheaccuracyandeﬃciencyofthesubsequentminin gprocess. Datapre-
8 CHAPTER 3. DATA PREPROCESSING
processingisanimportantstepintheknowledgediscoveryp rocess,becausequality
decisionsmustbebasedonqualitydata. Detectingdataanom alies,rectifyingthem
early, and reducing the data to be analyzed can lead to huge pa yoﬀs for decision
making.
3.2 Data Cleaning
Real-world data tend to be incomplete, noisy, and inconsist ent.Data cleaning
(ordata cleansing ) routines attempt to ﬁll in missing values, smooth out noise
while identifying outliers, and correct inconsistencies i n the data. In this section,
you will study basic methods for data cleaning. Section 3.2. 1 looks at ways
of handling missing values. Section 3.2.2 explains data smo othing techniques.
Section 3.2.3 discusses approaches to data cleaning as a pro cess.
3.2.1 Missing Values
Imaginethatyouneedtoanalyze AllElectronics salesandcustomerdata. Younote
that many tuples have no recorded value for several attribut es, such as customer
income . How can you go about ﬁlling in the missing values for this att ribute? Let’s
look at the following methods:
1.Ignore the tuple : This is usually done when the class label is missing
(assuming the mining task involves classiﬁcation). This me thod is not
very eﬀective, unless the tuple contains several attribute s with missing
values. It is especially poor when the percentage of missing values per
attribute varies considerably. By ignoring the tuple, we do not make use
of the remaining attributes values in the tuple. Such data co uld have been
useful to the task at hand.
2.Fill in the missing value manually : In general, this approach is time-
consuming and may not be feasible given a large data set with m any
missing values.
3.Use a global constant to ﬁll in the missing value : Replace all missing
attribute values by the same constant, such as a label like “Unknown” or
−∞. If missing values are replaced by, say, “Unknown,” then the mining
program may mistakenly think that they form an interesting c oncept, since
they all have a value in common—that of “Unknown.” Hence, although
this method is simple, it is not foolproof.
4.Use a measure of central tendency for the attribute (such as
the mean or median) to ﬁll in the missing value : Chapter 2 dis-
cussed measures of central tendency, which indicate the “mi ddle” value of
a data distribution. For normal (symmetric) data distribut ions, the mean
can be used, while skewed data distribution should employ th e median
(Section 2.2). For example, suppose that the data distribut ion regarding
3.2. DATA CLEANING 9
the income of AllElectronics customers is symmetric and that the average
income is $56,000. Use this value to replace the missing valu e forincome .
5.Use the attribute mean or median for all samples belonging to
the same class as the given tuple : For example, if classifying cus-
tomers according to credit risk, we may replace the missing value with
the average income value for customers in the same credit risk category
as that of the given tuple. If the data distribution for a give n class is
skewed, the median value is a better choice.
6.Use the most probable value to ﬁll in the missing value : This may
be determined with regression, inference-based tools usin g a Bayesian for-
malism, or decision tree induction. For example, using the o ther customer
attributes in your data set, you may construct a decision tre e to predict
the missing values for income . Decision trees and Bayesian inference are
described in detail in Chapters 8 and 9, respectively, while regression is
introduced in Section 3.4.5.
Methods 3 to 6 bias the data. The ﬁlled-in value may not be corr ect. Method
6, however, is a popular strategy. In comparison to the other methods, it uses
the most information from the present data to predict missin g values. By con-
sidering the values of the other attributes in its estimatio n of the missing value
forincome , there is a greater chance that the relationships between income and
the other attributes are preserved.
It is important to note that, in some cases, a missing value ma y not imply
an error in the data! For example, when applying for a credit c ard, candidates
may be asked to supply their driver’s license number. Candid ates who do not
have a driver’s license may naturally leave this ﬁeld blank. Forms should allow
respondents to specify values such as “not applicable”. Sof tware routines may
also be used to uncover other null values, such as “don’t know ”, “?”, or “none”.
Ideally, each attribute should have one or more rules regard ing the nullcondi-
tion. The rules may specify whether or not nulls are allowed, and/or how such
values should be handled or transformed. Fields may also be i ntentionally left
blank if they are to be provided in a later step of the business process. Hence,
although we can try our best to clean the data after it is seize d, good design
of databases and of data entry procedures should help minimi ze the number of
missing values or errors in the ﬁrst place.
3.2.2 Noisy Data
“What is noise?” Noise is a random error or variance in a measured variable.
In Chapter 2, we saw how some basic statistical description t echniques (such as
boxplots and scatter plots) and methods of data visualizati on can be used to
identify outliers, which may represent noise. Given a numer ic attribute such as,
say,price, how can we “smooth” out the data to remove the noise? Let’s lo ok
at the following data smoothing techniques:
1.Binning: Binning methods smooth a sorted data value by consulting its
“neighborhood,” that is, the values around it. The sorted va lues are dis-
10 CHAPTER 3. DATA PREPROCESSING
Sorted data for price (in dollars) : 4, 8, 15, 21, 21, 24, 25, 28, 34
Partition into (equal-frequency) bins :
Bin 1: 4, 8, 15
Bin 2: 21, 21, 24
Bin 3: 25, 28, 34
Smoothing by bin means :
Bin 1: 9, 9, 9
Bin 2: 22, 22, 22
Bin 3: 29, 29, 29
Smoothing by bin boundaries :
Bin 1: 4, 4, 15
Bin 2: 21, 21, 24
Bin 3: 25, 25, 34
Figure 3.2: Binning methods for data smoothing.
tributed into a number of “buckets,” or bins. Because binning methods
consult the neighborhood of values, they perform localsmoothing. Fig-
ure 3.2 illustrates some binning techniques. In this exampl e, the data for
priceare ﬁrst sorted and then partitioned into equal-frequency bins of size
3 (i.e., each bin contains three values). In smoothing by bin means ,
each value in a bin is replaced by the mean value of the bin. For example,
the mean of the values 4, 8, and 15 in Bin 1 is 9. Therefore, each original
value in this bin is replaced by the value 9. Similarly, smoothing by
bin medians can be employed, in which each bin value is replaced by
the bin median. In smoothing by bin boundaries , the minimum and
maximum values in a given bin are identiﬁed as the bin boundaries . Each
bin value is then replaced by the closest boundary value. In g eneral, the
larger the width, the greater the eﬀect of the smoothing. Alt ernatively,
bins may be equal-width , where the interval range of values in each bin is
constant. Binning is also used as a discretization techniqu e and is further
discussed in Section 3.5.
2.Regression: Data smoothing can also be done by conforming data values
to a function, a technique known as regression. Linear regression involves
ﬁnding the “best” line to ﬁt two attributes (or variables), s o that one
attribute can be used to predict the other. Multiple linear regression
is an extension of linear regression, where more than two att ributes are
involved and the data are ﬁt to a multidimensional surface. R egression is
further described in Section 3.4.5.
3.Outlier analysis : Outliers may be detected by clustering, for example,
3.2. DATA CLEANING 11
Figure 3.3: A 2-D plot of customer data with respect to custom er locations
in a city, showing three data clusters. Each cluster centroi d is marked with a
“+”, representing the average point in space for that cluste r. Outliers may be
detected as values that fall outside of the sets of clusters.
where similar values are organized into groups, or “cluster s.” Intuitively,
values that fall outside of the set of clusters may be conside red outliers
(Figure 3.3). Chapter 11 is dedicated to the topic of outlier analysis.
Many methods for data smoothing are also methods for data dis cretization
(a form of data transformation) and data reduction. For exam ple, the binning
techniques described above reduce the number of distinct va lues per attribute.
This acts as a form of data reduction for logic-based data min ing methods, such
as decision tree induction, which repeatedly make value com parisons on sorted
data. Concept hierarchies are a form of data discretization that can also be
used for data smoothing. A concept hierarchy for price, for example, may map
realprice values into inexpensive, moderately priced , and expensive , thereby
reducing the number of data values to be handled by the mining process. Data
discretization is discussed in Section 3.5. Some methods of classiﬁcation, such
as neural networks, have built-in data smoothing mechanism s. Classiﬁcation is
the topic of Chapters 8 and 9.
12 CHAPTER 3. DATA PREPROCESSING
3.2.3 Data Cleaning as a Process
Missing values, noise, and inconsistencies contribute to i naccurate data. So far,
we have looked at techniques for handling missing data and fo r smoothing data.
“But data cleaning is a big job. What about data cleaning as a p rocess? How
exactly does one proceed in tackling this task? Are there any tools out there to
help?”
The ﬁrst step in data cleaning as a process is discrepancy detection . Discrepan-
cies can be caused by several factors, including poorly desi gned data entry forms
that have many optional ﬁelds, human error in data entry, del iberate errors (e.g.,
respondentsnotwantingtodivulgeinformationaboutthems elves),anddatadecay
(e.g.,outdatedaddresses). Discrepanciesmayalsoarisef rominconsistentdatarep-
resentations and the inconsistent use of codes. Errors in in strumentation devices
thatrecorddata,andsystemerrors,areanothersourceofdi screpancies. Errorscan
alsooccurwhenthedataare(inadequately)usedforpurpose sotherthanoriginally
intended. There may also be inconsistencies due to data inte gration (e.g., where a
given attribute can have diﬀerent names in diﬀerent databas es).2
“So, how can we proceed with discrepancy detection?” As a starting point,
use any knowledge you may already have regarding properties of the data. Such
knowledge or “data about data” is referred to as metadata . This is where we
can make use of the knowledge we gained about our data in Chapt er 2. For
example, what are the data type and domain of each attribute? What are the
acceptable values for each attribute? The basic statistica l data descriptions dis-
cussed in Section 2.2 are useful here to grasp data trends and identify anomalies.
For example, ﬁnd the mean, median, and mode values. Are the da ta symmetric
or skewed? What is the range of values? Do all values fall with in the expected
range? What is the standard deviation of each attribute? Val ues that are more
than two standard deviations away from the mean for a given at tribute may
be ﬂagged as potential outliers. Are there any known depende ncies between
attributes? In this step, you may write your own scripts and/ or use some of
the tools that we discuss further below. From this, you may ﬁn d noise, outliers,
and unusual values that need investigation.
As adata analyst, youshould be on the lookoutfor the inconsi stentuse ofcodes
andanyinconsistentdatarepresentations(suchas“2010/1 2/25”and“25/12/2010”
fordate).Field overloading is another source of errors that typically results
when developers squeeze new attribute deﬁnitions into unus ed (bit) portions of
already deﬁned attributes (e.g., using an unused bit of an at tribute whose value
range uses only, say, 31 out of 32 bits).
The data should also be examined regarding unique rules, con secutive rules,
and null rules. A unique rule says that each value of the given attribute must
be diﬀerent from all other values for that attribute. A consecutive rule says
that there can be no missing values between the lowest and hig hest values for the
attribute, and that all values must also be unique (e.g., as i n check numbers).
Anull rule speciﬁes the use of blanks, question marks, special charact ers, or
2Data integration and the removal of redundant data that can r esult from such integration
are further described in Section 3.3.
3.2. DATA CLEANING 13
other strings that may indicate the null condition (e.g., wh ere a value for a
given attribute is not available), and how such values shoul d be handled. As
mentioned in Section 3.2.1, reasons for missing values may i nclude (1) the person
originally asked to provide a value for the attribute refuse s and/or ﬁnds that
the information requested is not applicable (e.g., a license-number attribute
left blank by nondrivers); (2) the data entry person does not know the correct
value; or (3) the value is to be provided by a later step of the p rocess. The null
rule should specify how to record the null condition, for exa mple, such as to
store zero for numeric attributes, a blank for character att ributes, or any other
conventions that may be in use (such as that entries like “don ’t know” or “?”
should be transformed to blank).
There are a number of diﬀerent commercial tools that can aid i n the step of
discrepancy detection. Data scrubbing tools use simple domain knowledge
(e.g., knowledge of postal addresses, and spell-checking) to detect errors and
make corrections in the data. These tools rely on parsing and fuzzy matching
techniques when cleaning data from multiple sources. Data auditing tools
ﬁnd discrepancies by analyzing the data to discover rules an d relationships, and
detecting data that violate such conditions. They are varia nts of data mining
tools. For example, they may employ statistical analysis to ﬁnd correlations,
or clustering to identify outliers. They may also use the bas ic statistical data
descriptions presented in Section 2.2.
Some data inconsistencies may be corrected manually using e xternal refer-
ences. For example, errors made at data entry may be correcte d by performing
a paper trace. Most errors, however, will require data transformations . That is,
once we ﬁnd discrepancies, we typically need to deﬁne and app ly (a series of)
transformations to correct them.
Commercial tools can assist in the data transformation step .Data mi-
gration tools allow simple transformations to be speciﬁed, such as to repl ace
the string “gender” by“sex”.ETL (extraction/transformation/loading)
tools allow users to specify transforms through a graphical user i nterface (GUI).
These tools typically support only a restricted set of trans forms so that, often,
we may also choose to write custom scripts for this step of the data cleaning
process.
The two-step process of discrepancy detection and data tran sformation (to
correct discrepancies) iterates. This process, however, i s error-prone and time-
consuming. Some transformations may introduce more discre pancies. Some
nested discrepancies may only be detected after others have been ﬁxed. For
example, a typo such as “20010” in a year ﬁeld may only surface once all date
values have been converted to a uniform format. Transformat ions are often
done as a batch process while the user waits without feedback . Only after
the transformation is complete can the user go back and check that no new
anomalies have been created by mistake. Typically, numerou s iterations are
required before the user is satisﬁed. Any tuples that cannot be automatically
handled by a given transformation are typically written to a ﬁle without any
explanation regarding the reasoning behind their failure. As a result, the entire
data cleaning process also suﬀers from a lack of interactivi ty.
14 CHAPTER 3. DATA PREPROCESSING
New approaches to data cleaning emphasize increased intera ctivity. Potter’s
Wheel, for example, is a publicly available data cleaning to ol that integrates dis-
crepancy detection and transformation. Users gradually bu ild a series of trans-
formations by composing and debugging individual transfor mations, one step at a
time, on a spreadsheet-likeinterface. The transformation scan be speciﬁed graphi-
callyorby providingexamples. Results areshownimmediate ly onthe recordsthat
arevisible on the screen. The user can choose to undo the tran sformations, so that
transformations that introduced additional errors can be “ erased.” The tool
performs discrepancy checking automatically in the backgr ound on the latest
transformed view of the data. Users can gradually develop an d reﬁne transfor-
mations as discrepancies are found, leading to more eﬀectiv e and eﬃcient data
cleaning.
Another approach to increased interactivity in data cleani ng is the develop-
ment of declarative languages for the speciﬁcation of data t ransformation opera-
tors. Such work focuses on deﬁning powerful extensions to SQ L and algorithms
that enable users to express data cleaning speciﬁcations eﬃ ciently.
As we discover more about the data, it is important to keep upd ating the
metadata to reﬂect this knowledge. This will help speed up da ta cleaning on
future versions of the same data store.
3.3 Data Integration
Data mining often requires data integration—the merging of data from multiple
data stores. Careful integration can help reduce and avoid r edundancies and
inconsistencies in the resulting data set. This can help imp rove the accuracy
and speed of the subsequent mining process.
The semantic heterogeneity and structure of data pose great challenges in
data integration. How can we match schema and objects from di ﬀerent sources?
This is the essence of the entity identiﬁcation problem , described in Section 3.3.1.
Are any attributes correlated? Section 3.3.2 presents corr elation tests for nu-
meric and nominal data. Tuple duplication is described in Se ction 3.3.3. Finally,
Section 3.3.4 touches on the detection and resolution of dat a value conﬂicts.
3.3.1 The Entity Identiﬁcation Problem
It is likely that your data analysis task will involve data integration , which
combines data from multiple sources into a coherent data sto re, as in data
warehousing. These sources may include multiple databases , data cubes, or ﬂat
ﬁles.
There are a number of issues to consider during data integrat ion.Schema
integration andobject matching can be tricky. How can equivalent real-world
entities from multiple data sources be matched up? This is re ferred to as the
entity identiﬁcation problem . For example, how can the data analyst or
the computer be sure that customer idin one database and custnumber in
another refer to the same attribute? Examples of metadata fo r each attribute
3.3. DATA INTEGRATION 15
include the name, meaning, data type, and range of values per mitted for the
attribute, and null rules for handling blank, zero, or null v alues (Section 3.2).
Such metadata can be used to help avoid errors in schema integ ration. The
metadata may also be used to help transform the data (e.g., wh ere data codes
forpaytypein one database may be “H”and“S”, and1and2in another).
Hence, this step also relates to data cleaning, as described earlier.
When matching attributes from one database to another durin g integration,
special attention must be paid to the structure of the data. This is to ensure that
any attribute functional dependencies and referential con straints in the source
system match those in the target system. For example, in one s ystem, a discount
may be applied to the order, whereas in another system it is ap plied to each
individual line item within the order. If this is not caught b efore integration,
items in the target system may be improperly discounted.
3.3.2 Redundancy and Correlation Analysis
Redundancy is another important issue in data integration. An attribut e (such
asannual revenue , for instance) may be redundant if it can be “derived” from
another attribute or set of attributes. Inconsistencies in attribute or dimension
naming can also cause redundancies in the resulting data set .
Some redundancies can be detected by correlation analysis . Given two
attributes, such analysis can measure how strongly one attr ibute implies the
other, based on the available data. For nominal data, we use t heχ2(chi-
square ) test. For numeric attributes, we can use the correlation coeﬃcient and
covariance , both of which access how one attribute’s values vary with th ose of
another.
χ2Correlation Test for Nominal Data
For nominal data, a correlation relationship between two at tributes, AandB,
can be discovered by a χ2(chi-square ) test. Suppose Ahascdistinct values,
namely a1, a2, . . . a c.Bhasrdistinct values, namely b1, b2, . . . b r. The data
tuples described by AandBcan be shown as a contingency table , with the
cvalues of Amaking up the columns and the rvalues of Bmaking up the
rows. Let ( Ai, Bj) denote the joint event that attribute Atakes on value aiand
attribute Btakes on value bj, that is, where ( A=ai, B=bj). Each and every
possible ( Ai, Bj) joint event has its own cell (or slot) in the table. The χ2value
(also known as the Pearson χ2statistic ) is computed as:
χ2=c/summationdisplay
i=1r/summationdisplay
j=1(oij−eij)2
eij, (3.1)
where oijis theobserved frequency (i.e., actual count) of the joint event ( Ai, Bj)
andeijis the expected frequency of (Ai, Bj), which can be computed as
eij=count(A=ai)×count(B=bj)
n, (3.2)
16 CHAPTER 3. DATA PREPROCESSING
Table 3.1: A 2 ×2 contingency table for the data
of Example 2.1. Are gender andpreferred Reading
correlated?
male female Total
ﬁction 250 (90) 200 (360) 450
nonﬁction 50 (210) 1000 (840) 1050
Total 300 1200 1500
where nis the number of data tuples, count(A=ai) is the number of tuples
having value aiforA, and count(B=bj) is the number of tuples having value
bjforB. The sum in Equation (3.1) is computed over all of the r×ccells. Note
that the cells that contribute the most to the χ2value are those whose actual
count is very diﬀerent from that expected.
Theχ2statistic tests the hypothesis that AandBareindependent , that is,
there is no correlation between them. The test is based on a si gniﬁcance level,
with ( r−1)×(c−1) degrees of freedom. We will illustrate the use of this
statistic in an example below. If the hypothesis can be rejec ted, then we say
thatAandBare statistically correlated.
Let’s look at a concrete example.
Example 3.1 Correlation analysis of nominal attributes using χ2.Suppose that a
group of 1,500 people was surveyed. The gender of each person was noted. Each
person was polled as to whether their preferred type of readi ng material was ﬁc-
tion or nonﬁction. Thus, we have two attributes, gender andpreferred reading .
The observed frequency (or count) of each possible joint eve nt is summarized
in the contingency table shown in Table 3.1, where the number s in parentheses
are the expected frequencies. The expected frequencies are calculated based on
the data distribution for both attributes using Equation (3 .2).
Using Equation (3.2), we can verify the expected frequencie s for each cell.
For example, the expected frequency for the cell (male, ﬁcti on) is
e11=count(male )×count(ﬁction )
n=300×450
1500= 90,
and so on. Notice that in any row, the sum of the expected frequ encies must
equal the total observed frequency for that row, and the sum o f the expected
frequencies in any column must also equal the total observed frequency for that
column. Using Equation (3.1) for χ2computation, we get
χ2=(250−90)2
90+(50−210)2
210+(200−360)2
360+(1000 −840)2
840
= 284 .44 + 121 .90 + 71 .11 + 30 .48 = 507 .93.
For this 2 ×2 table, the degrees of freedom are (2 −1)(2−1) = 1. For 1
degree of freedom, the χ2value needed to reject the hypothesis at the 0.001
signiﬁcance level is 10.828 (taken from the table of upper pe rcentage points of
3.3. DATA INTEGRATION 17
theχ2distribution, typically available from any textbook on sta tistics). Since
our computed value is above this, we can reject the hypothesi s that gender
andpreferred reading are independent and conclude that the two attributes are
(strongly) correlated for the given group of people.
Correlation Coeﬃcient for Numeric Data
For numeric attributes, we can evaluate the correlation bet ween two attributes,
AandB, by computing the correlation coeﬃcient (also known as Pearson’s
product moment coeﬃcient , named after its inventer, Karl Pearson). This
is
rA,B=n/summationdisplay
i=1(ai−¯A)(bi−¯B)
nσAσB=n/summationdisplay
i=1(aibi)−n¯A¯B
nσAσB, (3.3)
where nis the number of tuples, aiandbiare the respective values of AandB
in tuple i,¯Aand¯Bare the respective mean values of AandB,σAandσBare
the respective standard deviations of AandB(as deﬁned in Section 2.2.8), and
Σ(aibi) is the sum of the ABcross-product (that is, for each tuple, the value for
Ais multiplied by the value for Bin that tuple). Note that −1≤rA,B≤+1.
IfrA,Bis greater than 0, then AandBarepositively correlated , meaning that
the values of Aincrease as the values of Bincrease. The higher the value, the
stronger the correlation (i.e., the more each attribute imp lies the other). Hence,
a higher value may indicate that A(orB) may be removed as a redundancy. If
the resulting value is equal to 0, then AandBareindependent and there is no
correlation between them. If the resulting value is less tha n 0, then AandB
arenegatively correlated , where the values of one attribute increase as the values
of the other attribute decrease. This means that each attrib ute discourages the
other. Scatter plots can also be used to view correlations be tween attributes
(Section 2.2.12). For example, the scatter plots of Figure 2 .9 respectively show
positively correlated data and negatively correlated data , while Figure 2.10 dis-
plays uncorrelated data.
Note that correlation does not imply causality. That is, if AandBare
correlated, this does not necessarily imply that Acauses Bor that Bcauses A.
For example, in analyzing a demographic database, we may ﬁnd that attributes
representing the number of hospitals and the number of car th efts in a region
are correlated. This does not mean that one causes the other. Both are actually
causally linked to a third attribute, namely, population .
Covariance of Numeric Data
In probability theory and statistics, correlation and cova riance are two similar
measures for assessing how much two attributes change toget her. Consider two
numeric attributes AandB, and a set of nobservations {(a1, b1), . . . ,(an, bn)}.
The mean values of AandB, respectively, are also known as the expected
18 CHAPTER 3. DATA PREPROCESSING
Time point AllElectronics HighTech
t1 6 20
t2 5 10
t3 4 14
t4 3 5
t5 2 5
Table 3.2: Stock prices for AllElectronics andHighTech .
values onAandB, that is,
E(A) =¯A=/summationtextn
i=1ai
n
and
E(B) =¯B=/summationtextn
i=1bi
n.
Thecovariance between AandBis deﬁned as
Cov(A, B) =E((A−¯A)(B−¯B)) =/summationtextn
i=1(ai−¯A)(bi−¯B)
n. (3.4)
If we compare Equation 3.3 for rA,B(correlation coeﬃcient) with Equa-
tion 3.4 for covariance, we see that
rA,B=Cov(A, B)
σAσB(3.5)
where σAandσBare the standard deviations of AandB, respectively. It can
also be shown that
Cov(A, B) =E(A·B)−¯A¯B. (3.6)
This equation may simplify calculations.
For two attributes AandBthat tend to change together, if Ais larger than
¯A(the expected value of A), then Bis likely to be larger than ¯B(the expected
value of B). Therefore, the covariance between AandBispositive . On the
other hand, if one of the attributes tends to be above its expe cted value when
the other attribute is below its expected value, then the cov ariance of AandB
isnegative .
IfAandBareindependent , that is, they do not have correlation, then E(A·
B) =E(A)·E(B).Therefore, the covariance is Cov(A, B) =E(A·B)−¯A¯B=
E(A)·E(B)−¯A¯B= 0.However, the converse is not true. Some pairs of random
variables (attributes) may have a covariance of 0 but are not independent. Only
under some additional assumptions (such as that the data fol low multivariate
normal distributions) does a covariance of 0 imply independ ence.
Example 3.2 Covariance analysis of numeric attributes. Consider Table 3.2, which
presents a simpliﬁed example of stock prices observed at ﬁve time points for
3.3. DATA INTEGRATION 19
AllElectronics andHighTech , some high tech company. If the stocks are aﬀected
by the same industry trends, will their prices rise or fall to gether?
E(AllElectronics ) =6 + 5 + 4 + 3 + 2
5=20
5= $4
and
E(HighTech ) =20 + 10 + 14 + 5 + 5
5=54
5= $10 .8.
Thus, using Equation 3.4, we compute
Cov(AllElectroncis, HighTech ) =6×20+5×10+4×14+3×5+2×5
5−4×10.8
= 50 .2−43.2 = 7.
Therefore, given the positive covariance we can say that sto ck prices for both
companies rise together.
Variance is a special case of covariance, where the two attributes are identical
(that is, the covariance of an attribute with itself). Varia nce was discussed in
Chapter 2.
3.3.3 Tuple Duplication
In addition to detecting redundancies between attributes, duplication should
also be detected at the tuple level (e.g., where there are two or more identical
tuples for a given unique data entry case). The use of denorma lized tables (often
done to improve performance by avoiding joins) is another source of data redun-
dancy. Inconsistencies often arise between various duplic ates, due to inaccurate
data entry or updating some but not all of the occurrences of t he data. For
example, if a purchase order database contains attributes f or the purchaser’s
name and address instead of a key to this information in a purc haser database,
discrepancies can occur, such as the same purchaser’s name a ppearing with
diﬀerent addresses within the purchase order database.
3.3.4 Detection and Resolution of Data Value Conﬂicts
Data integration also involves the detection and resolution of data value con-
ﬂicts. For example, for the same real-world entity, attribute val ues from dif-
ferent sources may diﬀer. This may be due to diﬀerences in rep resentation,
scaling, or encoding. For instance, a weight attribute may be stored in metric
units in one system and British imperial units in another. Fo r a hotel chain,
theprice of rooms in diﬀerent cities may involve not only diﬀerent cur rencies
but also diﬀerent services (such as free breakfast) and taxe s. When exchang-
ing information between schools, each school may have its ow n curriculum and
grading scheme. One university may adopt a quarter system, o ﬀer three courses
on database systems, and assign grades from A+ to F, whereas a nother may
adopt a semester system, oﬀer two courses on databases, and a ssign grades from
20 CHAPTER 3. DATA PREPROCESSING
1 to 10. It is diﬃcult to work out precise course-to-grade tra nsformation rules
between the two universities, making information exchange diﬃcult.
Attributes may also diﬀer on the level of abstraction, where an attribute
in one system is recorded at, say, a lower level of abstractio n than the “same”
attribute in another. For example, the totalsalesin one database may refer to
one branch of AllElectronics , while an attribute of the same name in another
database may refer to the total sales for AllElectronics stores in a given region.
The topic of discrepancy detection is further described in S ection 3.2.3 on data
cleaning as a process.
3.4 Data Reduction
Imagine that you have selected data from the AllElectronics data warehouse for
analysis. The data set will likely be huge! Complex data anal ysis and mining
on huge amounts of data can take a long time, making such analy sis impractical
or infeasible.
Data reduction techniques can be applied to obtain a reduced representa-
tion of the data set that is much smaller in volume, yet closel y maintains the
integrity of the original data. That is, mining on the reduce d data set should be
more eﬃcient yet produce the same (or almost the same) analyt ical results. In
this section, we ﬁrst present an overview of data reduction s trategies, followed
by a closer look at individual techniques.
3.4.1 Overview of Data Reduction Strategies
Data reduction strategies include dimensionality reduction ,numerosity reduc-
tion, anddata compression .
Dimensionality reduction is the process of reducing the number of ran-
dom variables or attributes under consideration. Dimensio nality reduction meth-
ods include wavelet transforms (Section 3.4.2) and principal components analy-
sis(Section 3.4.3), which transform or project the original da ta onto a smaller
space. Attribute subset selection is a method of dimensionality reduction in
which irrelevant, weakly relevant, or redundant attribute s or dimensions are
detected and removed (Section 3.4.4).
Techniques of numerosity reduction replace the original data volume by
alternative, smaller forms of data representation. These t echniques may be para-
metric or nonparametric. For parametric methods , a model is used to estimate
the data, so that typically only the data parameters need to b e stored, instead of
the actual data. (Outliers may also be stored.) Regression a nd log-linear mod-
els (Section 3.4.5) are examples. Nonparametric methods for storing reduced
representations of the data include histograms (Section 3.4.6), clustering (Sec-
tion 3.4.7), sampling (Section 3.4.8), and data cube aggregation (Section 3.4.9).
Indata compression , transformations are applied so as to obtain a re-
duced or “compressed” representation of the original data. If the original data
can be reconstructed from the compressed data without any loss of information,
3.4. DATA REDUCTION 21
the data reduction is called lossless . If, instead, we can reconstruct only an ap-
proximation of the original data, then the data reduction is calledlossy. There
are several lossless algorithms for string compression, ho wever, they typically
allow only limited manipulation of the data. Dimensionalit y reduction and nu-
merosity reduction techniques can also be considered forms of data compression.
There are many other ways of organizing methods of data reduc tion. The
computational time spent on data reduction should not outwe igh or “erase” the
time saved by mining on a reduced data set size.
3.4.2 Wavelet Transforms
Thediscrete wavelet transform (DWT) is a linear signal processing tech-
nique that, when applied to a data vector X, transforms it to a numerically
diﬀerent vector, X′, ofwavelet coeﬃcients . The two vectors are of the same
length. When applying this technique to data reduction, we c onsider each tu-
ple as an n-dimensional data vector, that is, X= (x1, x2, . . . , x n), depicting n
measurements made on the tuple from ndatabase attributes.3
“How can this technique be useful for data reduction if the wa velet trans-
formed data are of the same length as the original data?” The usefulness lies
in the fact that the wavelet transformed data can be truncate d. A compressed
approximation of the data can be retained by storing only a sm all fraction of
the strongest of the wavelet coeﬃcients. For example, all wa velet coeﬃcients
larger than some user-speciﬁed threshold can be retained. A ll other coeﬃcients
are set to 0. The resulting data representation is therefore very sparse, so that
operations that can take advantage of data sparsity are comp utationally very
fast if performed in wavelet space. The technique also works to remove noise
without smoothing out the main features of the data, making i t eﬀective for data
cleaning as well. Given a set of coeﬃcients, an approximatio n of the original
data can be constructed by applying the inverse of the DWT used.
The DWT is closely related to the discrete Fourier transform (DFT) , a signal
processing technique involving sines and cosines. In gener al, however, the DWT
achieves better lossy compression. That is, if the same numb er of coeﬃcients
is retained for a DWT and a DFT of a given data vector, the DWT ve rsion
will provide a more accurate approximation of the original d ata. Hence, for an
equivalent approximation, the DWT requires less space than the DFT. Unlike
the DFT, wavelets are quite localized in space, contributin g to the conservation
of local detail.
There is only one DFT, yet there are several families of DWTs. Figure 3.4
shows some wavelet families. Popular wavelet transforms in clude the Haar-2,
Daubechies-4, and Daubechies-6 transforms. The general pr ocedure for applying
a discrete wavelet transform uses a hierarchical pyramid algorithm that halves
the data at each iteration, resulting in fast computational speed. The method
is as follows:
3In our notation, any variable representing a vector is shown in bold italic font; measure-
ments depicting the vector are shown in italic font.
22 CHAPTER 3. DATA PREPROCESSING
/H110021.0/H110020.5 0.0 0.5 1.0 1.5 2.0 0 2 4 6
(a) Haar-2 (b) Daubechies-40.8
0.6
0.4
0.2
0.00.6
0.4
0.2
0.0
Figure 3.4: Examples of wavelet families. The number next to a wavelet name is
the number of vanishing moments of the wavelet. This is a set of mathematical
relationships that the coeﬃcients must satisfy and is relat ed to the number of
coeﬃcients.
1. The length, L, of the input data vector must be an integer power of 2. This
condition can be met by padding the data vector with zeros as n ecessary
(L≥n).
2. Each transform involves applying two functions. The ﬁrst applies some
data smoothing, such as a sum or weighted average. The second performs
a weighted diﬀerence, which acts to bring out the detailed fe atures of the
data.
3. The two functions are applied to pairs of data points in X, that is, to
all pairs of measurements ( x2i, x2i+1). This results in two sets of data
of length L/2. In general, these represent a smoothed or low-frequency
version of the input data and the high-frequency content of i t, respectively.
4. The two functions are recursively applied to the sets of da ta obtained in
the previous loop, until the resulting data sets obtained ar e of length 2.
5. Selected values from the data sets obtained in the above it erations are
designated the wavelet coeﬃcients of the transformed data.
Equivalently, a matrix multiplication can be applied to the input data in
order to obtain the wavelet coeﬃcients, where the matrix use d depends on the
given DWT. The matrix must be orthonormal , meaning that the columns are
unit vectors and are mutually orthogonal, so that the matrix inverse is just
its transpose. Although we do not have room to discuss it here , this property
allows the reconstruction of the data from the smooth and smo oth-diﬀerence
data sets. By factoring the matrix used into a product of a few sparse matrices,
the resulting “fast DWT” algorithm has a complexity of O(n) for an input
vector of length n.
Wavelet transforms can be applied to multidimensional data , such as a data
cube. This is done by ﬁrst applying the transform to the ﬁrst d imension, then
3.4. DATA REDUCTION 23
to the second, and so on. The computational complexity invol ved is linear with
respect to the number of cells in the cube. Wavelet transform s give good results
on sparse or skewed data and on data with ordered attributes. Lossy compression
by wavelets is reportedly better than JPEG compression, the current commercial
standard. Wavelet transforms have many real-world applica tions, including the
compression of ﬁngerprint images, computer vision, analys is of time-series data,
and data cleaning.
3.4.3 Principal Components Analysis
In this subsection we provide an intuitive introduction to p rincipal components
analysis as a method of dimesionality reduction. A detailed theoretical expla-
nation is beyond the scope of this book. For additional refer ences, please see
the bibliographic notes at the end of this chapter.
Suppose that the data to be reduced consist of tuples or data v ectors de-
scribed by nattributes or dimensions. Principal components analysis , or
PCA (also called the Karhunen-Loeve, or K-L, method), searches fork n-
dimensional orthogonal vectors that can best be used to repr esent the data,
where k≤n. The original data are thus projected onto a much smaller spa ce,
resulting in dimensionality reduction. Unlike attribute s ubset selection (Sec-
tion 3.4.4), which reduces the attribute set size by retaini ng a subset of the
initial set of attributes, PCA “combines” the essence of att ributes by creating
an alternative, smaller set of variables. The initial data c an then be projected
onto this smaller set. PCA often reveals relationships that were not previously
suspected and thereby allows interpretations that would no t ordinarily result.
The basic procedure is as follows:
1. The input data are normalized, so that each attribute fall s within the same
range. This step helps ensure that attributes with large dom ains will not
dominate attributes with smaller domains.
2. PCA computes korthonormal vectors that provide a basis for the nor-
malized input data. These are unit vectors that each point in a direction
perpendicular to the others. These vectors are referred to a s theprinci-
pal components . The input data are a linear combination of the principal
components.
3. The principal components are sorted in order of decreasin g “signiﬁcance”
or strength. The principal components essentially serve as a new set of
axes for the data, providing important information about va riance. That
is, the sorted axes are such that the ﬁrst axis shows the most v ariance
among the data, the second axis shows the next highest varian ce, and so
on. For example, Figure 3.5 shows the ﬁrst two principal comp onents,Y1
andY2, for the given set of data originally mapped to the axes X1and
X2. This information helps identify groups or patterns within the data.
4. Because the components are sorted according to the decrea sing order of
“signiﬁcance,” the size of the data can be reduced by elimina ting the
24 CHAPTER 3. DATA PREPROCESSING
Y2X2
Y1
X1
Figure 3.5: Principal components analysis. Y1andY2are the ﬁrst two principal
components for the given data. NOTE: Figure needs to be corre cted so that
Y1andY2are orthogonal.
weaker components, that is, those with low variance. Using t he strongest
principal components, it should be possible to reconstruct a good approx-
imation of the original data.
PCA can be applied to ordered and unordered attributes, and c an handle
sparse data and skewed data. Multidimensional data of more t han two dimen-
sions can be handled by reducing the problem to two dimension s. Principal
components may be used as inputs to multiple regression and c luster analysis.
In comparison with wavelet transforms, PCA tends to be bette r at handling
sparse data, whereas wavelet transforms are more suitable f or data of high
dimensionality.
3.4.4 Attribute Subset Selection
Data sets for analysis may contain hundreds of attributes, m any of which may
be irrelevant to the mining task or redundant. For example, I f the task is to
classify customers based on a probability of positive react ion on a discount oﬀer
of a music CD, attributes such as the customer’s telephone nu mber are likely
to be irrelevant, unlike attributes such as ageormusic taste. Although it may
be possible for a domain expert to pick out some of the useful a ttributes, this
can be a diﬃcult and time-consuming task, especially when th e behavior of
the data is not well known (hence, a reason behind its analysi s!). Leaving out
relevant attributes or keeping irrelevant attributes may b e detrimental, causing
confusion for the mining algorithm employed. This can resul t in discovered pat-
terns of poor quality. In addition, the added volume of irrel evant or redundant
attributes can slow down the mining process.
Attribute subset selection4reduces the data set size by removing irrel-
evant or redundant attributes (or dimensions). The goal of a ttribute subset
selection is to ﬁnd a minimum set of attributes such that the r esulting prob-
ability distribution of the data classes is as close as possi ble to the original
distribution obtained using all attributes. Mining on a red uced set of attributes
4In machine learning, attribute subset selection is known as feature subset selection .
3.4. DATA REDUCTION 25
Forward selection
Initial attribute set:
{A1, A2, A3, A4, A5, A6}

Initial reduced set:
{}
=> {A1}
=> {A1, A4}
=> Reduced attribute set:
     {A1, A4, A6}
Initial attribute set:
{A1, A2, A3, A4, A5, A6}

=> {A1, A3, A4, A5, A6}
=> {A1, A4, A5, A6}
=> Reduced attribute set:
     {A1, A4, A6}
Initial attribute set:
{A1, A2, A3, A4, A5, A6}
=> Reduced attribute set:
     {A1, A4, A6}
Backward elimination Decision tree induction

A4?
A1? A6?
Class 1 Class 2 Class 1 Class 2Y N
Y N Y N
Figure 3.6: Greedy (heuristic) methods for attribute subse t selection.
has an additional beneﬁt. It reduces the number of attribute s appearing in the
discovered patterns, helping to make the patterns easier to understand.
“How can we ﬁnd a ‘good’ subset of the original attributes?” Fornattributes,
there are 2npossible subsets. An exhaustive search for the optimal subs et of
attributes can be prohibitively expensive, especially as nand the number of data
classes increase. Therefore, heuristic methods that explo re a reduced search
space are commonly used for attribute subset selection. The se methods are
typically greedy in that, while searching through attribute space, they alwa ys
make what looks to be the best choice at the time. Their strate gy is to make
a locally optimal choice in the hope that this will lead to a gl obally optimal
solution. Such greedy methods are eﬀective in practice and m ay come close to
estimating an optimal solution.
The “best” (and “worst”) attributes are typically determin ed using tests of
statistical signiﬁcance, which assume that the attributes are independent of one
another. Many other attribute evaluation measures can be us ed, such as the
information gain measure used in building decision trees for classiﬁcation.5
Basic heuristic methods of attribute subset selection incl ude the following
techniques, some of which are illustrated in Figure 3.6.
1.Stepwise forward selection : The procedure starts with an empty set
of attributes as the reduced set. The best of the original att ributes is
determined and added to the reduced set. At each subsequent i teration
or step, the best of the remaining original attributes is add ed to the set.
2.Stepwise backward elimination : The procedure starts with the full
set of attributes. At each step, it removes the worst attribu te remaining
in the set.
5The information gain measure is described in detail in Chapt er 8.
26 CHAPTER 3. DATA PREPROCESSING
3.Combination of forward selection and backward elimination : The
stepwise forward selection and backward elimination metho ds can be com-
bined so that, at each step, the procedure selects the best at tribute and
removes the worst from among the remaining attributes.
4.Decision tree induction : Decision tree algorithms, such as ID3, C4.5,
and CART, were originally intended for classiﬁcation. Deci sion tree in-
duction constructs a ﬂowchart-like structure where each in ternal (nonleaf)
node denotes a test on an attribute, each branch corresponds to an out-
come of the test, and each external (leaf) node denotes a clas s prediction.
At each node, the algorithm chooses the “best” attribute to p artition the
data into individual classes.
When decision tree induction is used for attribute subset se lection, a tree
is constructed from the given data. All attributes that do no t appear in the
tree are assumed to be irrelevant. The set of attributes appe aring in the tree
form the reduced subset of attributes.
The stopping criteria for the methods may vary. The procedur e may em-
ploy a threshold on the measure used to determine when to stop the attribute
selection process.
In some cases, we may want to create new attributes based on ot hers. Such
attribute construction6can help improve accuracy and understanding of
structure in high-dimensional data. For example, we may wis h to add the at-
tribute areabased on the attributes height andwidth. By combining attributes,
attribute construction can discover missing information a bout the relationships
between data attributes that can be useful for knowledge dis covery.
3.4.5 Regression and Log-Linear Models: Parametric Data
Reduction
Regression and log-linear models can be used to approximate the given data.
In (simple) linear regression , the data are modeled to ﬁt a straight line. For
example, a random variable, y(called a response variable ), can be modeled as a
linear function of another random variable, x(called a predictor variable ), with
the equation
y=wx+b, (3.7)
where the variance of yis assumed to be constant. In the context of data
mining, xandyare numeric database attributes. The coeﬃcients, wandb
(called regression coeﬃcients ), specify the slope of the line and the Y-intercept,
respectively. These coeﬃcients can be solved for by the method of least squares ,
which minimizes the error between the actual line separatin g the data and the
estimate of the line. Multiple linear regression is an extension of (simple)
linear regression, which allows a response variable, y, to be modeled as a linear
function of two or more predictor variables.
6In the machine learning literature, attribute constructio n is known as feature construction .
3.4. DATA REDUCTION 27
Log-linear models approximate discrete multidimensional probability dis-
tributions. Given a set of tuples in ndimensions (e.g., described by nat-
tributes), we can consider each tuple as a point in an n-dimensional space.
Log-linear models can be used to estimate the probability of each point in a
multidimensional space for a set of discretized attributes , based on a smaller
subset of dimensional combinations. This allows a higher-d imensional data
space to be constructed from lower-dimensional spaces. Log -linear models are
therefore also useful for dimensionality reduction (since the lower-dimensional
points together typically occupy less space than the origin al data points) and
data smoothing (since aggregate estimates in the lower-dim ensional space are
less subject to sampling variations than the estimates in th e higher-dimensional
space).
Regression and log-linear models can both be used on sparse d ata, although
their application may be limited. While both methods can han dle skewed data,
regression does exceptionally well. Regression can be comp utationally intensive
when applied to high-dimensional data, whereas log-linear models show good
scalability for up to 10 or so dimensions.
Several software packages exist to solve regression proble ms. Examples in-
clude SAS ( www.sas.com ), SPSS ( www.spss.com ), and S-Plus ( www.insightful.com ).
Another useful resource is the book Numerical Recipes in C, by Press, Flannery,
Teukolsky, and Vetterling, and its associated source code.
3.4.6 Histograms
Histograms use binning to approximate data distributions a nd are a popu-
lar form of data reduction. Histograms were introduced in Se ction 2.2.9. A
histogram for an attribute, A, partitions the data distribution of Ainto dis-
joint subsets, or buckets . If each bucket represents only a single attribute-
value/frequency pair, the buckets are called singleton buckets . Often, buckets
instead represent continuous ranges for the given attribut e.
Example 3.3 Histograms. The following data are a list of prices of commonly sold items at
AllElectronics (rounded to the nearest dollar). The numbers have been sorte d:
1, 1, 5, 5, 5, 5, 5, 8, 8, 10, 10, 10, 10, 12, 14, 14, 14, 15, 15, 15, 15, 15, 15, 18,
18, 18, 18, 18, 18, 18, 18, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 25, 25, 25,
25, 25, 28, 28, 30, 30, 30.
Figure 3.7 shows a histogram for the data using singleton buc kets. To further
reduce the data, it is common to have each bucket denote a cont inuous range of
values for the given attribute. In Figure 3.8, each bucket re presents a diﬀerent
$10 range for price.
“How are the buckets determined and the attribute values par titioned?” There
are several partitioning rules, including the following:
•Equal-width : In an equal-width histogram, the width of each bucket
range is uniform (such as the width of $10 for the buckets in Fi gure 3.8).
28 CHAPTER 3. DATA PREPROCESSING
10
9
8
7
6
5
4
3
2
1
0
5 10 15 20 25 30
price ($)count
Figure 3.7: A histogram for price using singleton buckets—each bucket repre-
sents one price-value/ frequency pair.
•Equal-frequency (or equidepth): In an equal-frequency histogram, the
buckets are created so that, roughly, the frequency of each b ucket is con-
stant (that is, each bucket contains roughly the same number of contiguous
data samples).
Histograms are highly eﬀective at approximating both spars e and dense data,
as well as highly skewed and uniform data. The histograms des cribed above
for single attributes can be extended for multiple attribut es.Multidimensional
histograms can capture dependencies between attributes. Such histogr ams have
been found eﬀective in approximating data with up to ﬁve attr ibutes. More
studies are needed regarding the eﬀectiveness of multidime nsional histograms
for high dimensionalities.
Singleton buckets are useful for storing outliers with high frequency.
3.4.7 Clustering
Clustering techniques consider data tuples as objects. The y partition the objects
into groups or clusters , so that objects within a cluster are “similar” to one
another and “dissimilar” to objects in other clusters. Simi larity is commonly
deﬁned in terms of how “close” the objects are in space, based on a distance
function. The “quality” of a cluster may be represented by it sdiameter , the
maximum distance between any two objects in the cluster. Centroid distance is
an alternative measure of cluster quality and is deﬁned as th e average distance
3.4. DATA REDUCTION 29
25
20
15
10
5
0
1–10 11–20 21–30
price ($)count
Figure 3.8: An equal-width histogram for price, where values are aggregated so
that each bucket has a uniform width of $10.
of each cluster object from the cluster centroid (denoting t he “average object,”
or average point in space for the cluster). Figure 3.3 of Sect ion 3.2.2 shows a
2-D plot of customer data with respect to customer locations in a city, where
the centroid of each cluster is shown with a “+”. Three data cl usters are visible.
In data reduction, the cluster representations of the data a re used to replace
the actual data. The eﬀectiveness of this technique depends on the nature of
the data. It is much more eﬀective for data that can be organiz ed into distinct
clusters than for smeared data.
There are many measures for deﬁning clusters and cluster qua lity. Clustering
methods are further described in Chapters 10 and 11.
3.4.8 Sampling
Sampling can be used as a data reduction technique because it allows a large
data set to be represented by a much smaller random sample (or subset) of the
data. Suppose that a large data set, D, contains Ntuples. Let’s look at the
most common ways that we could sample Dfor data reduction, as illustrated
in Figure 3.9.
•Simple random sample without replacement (SRSWOR) of size
s: This is created by drawing sof the Ntuples from D(s < N ), where
the probability of drawing any tuple in Dis 1/N, that is, all tuples are
equally likely to be sampled.
•Simple random sample with replacement (SRSWR) of size s:
This is similar to SRSWOR, except that each time a tuple is dra wn from
D, it is recorded and then replaced . That is, after a tuple is drawn, it is
placed back in Dso that it may be drawn again.
30 CHAPTER 3. DATA PREPROCESSING
Figure 3.9: Sampling can be used for data reduction.
•Cluster sample : If the tuples in Dare grouped into Mmutually disjoint
“clusters,” then an SRS of sclusters can be obtained, where s < M . For
example, tuples in a database are usually retrieved a page at a time, so that
each page can be considered a cluster. A reduced data represe ntation can
be obtained by applying, say, SRSWOR to the pages, resulting in a cluster
sample of the tuples. Other clustering criteria conveying r ich semantics
can also be explored. For example, in a spatial database, we m ay choose
to deﬁne clusters geographically based on how closely diﬀer ent areas are
located.
•Stratiﬁed sample : IfDis divided into mutually disjoint parts called
strata, a stratiﬁed sample of Dis generated by obtaining an SRS at each
3.4. DATA REDUCTION 31
stratum. This helps ensure a representative sample, especi ally when the
data are skewed. For example, a stratiﬁed sample may be obtai ned from
customer data, where a stratum is created for each customer a ge group.
In this way, the age group having the smallest number of custo mers will
be sure to be represented.
An advantage of sampling for data reduction is that the cost o f obtaining a
sample is proportional to the size of the sample ,s, as opposed to N, the data set
size. Hence, sampling complexity is potentially sublinear to the size of the data.
Other data reduction techniques can require at least one com plete pass through
D. For a ﬁxed sample size, sampling complexity increases only linearly as the
number of data dimensions, n, increases, whereas techniques using histograms,
for example, increase exponentially in n.
When applied to data reduction, sampling is most commonly us ed to esti-
mate the answer to an aggregate query. It is possible (using t he central limit
theorem) to determine a suﬃcient sample size for estimating a given function
within a speciﬁed degree of error. This sample size, s, may be extremely small
in comparison to N. Sampling is a natural choice for the progressive reﬁnement
of a reduced data set. Such a set can be further reﬁned by simpl y increasing
the sample size.
3.4.9 Data Cube Aggregation
Imagine that you have collected the data for your analysis. T hese data consist
of the AllElectronics sales per quarter, for the years 2008 to 2010. You are,
however, interested in the annual sales (total per year), ra ther than the total per
quarter. Thus the data can be aggregated so that the resulting data summarize
the total sales per year instead of per quarter. This aggrega tion is illustrated
in Figure 3.10. The resulting data set is smaller in volume, w ithout loss of
information necessary for the analysis task.
Data cubes are discussed in detail in Chapter 4 on data wareho using and
Chapter 5 on advanced data cube technology. We brieﬂy introd uce some con-
cepts here. Data cubes store multidimensional aggregated i nformation. For
example, Figure 3.11 shows a data cube for multidimensional analysis of sales
data with respect to annual sales per item type for each AllElectronics branch.
Each cell holds an aggregate data value, corresponding to th e data point in
multidimensional space. (For readability, only some cell v alues are shown.)
Concept hierarchies may exist for each attribute, allowing the analysis of data
at multiple levels of abstraction. For example, a hierarchy forbranch could
allow branches to be grouped into regions, based on their add ress. Data cubes
provide fast access to precomputed, summarized data, there by beneﬁting on-
line analytical processing as well as data mining.
The cube created at the lowest level of abstraction is referr ed to as the base
cuboid. The base cuboid should correspond to an individual entity of interest,
such as salesorcustomer . In other words, the lowest level should be usable, or
useful for the analysis. A cube at the highest level of abstra ction is the apex
32 CHAPTER 3. DATA PREPROCESSING
QuarterYear 2004
Sales
Q1
Q2
Q3
Q4$224,000
$408,000
$350,000
$586,000QuarterYear 2003
Sales
Q1
Q2
Q3
Q4$224,000
$408,000
$350,000
$586,000QuarterYear 2002
Sales
Q1
Q2
Q3
Q4$224,000
$408,000
$350,000
$586,000Year Sales
2002
2003
2004$1,568,000
$2,356,000
$3,594,000
Figure 3.10: Sales data for a given branch of AllElectronics for the years 2002
to 2004. On the left, the sales are shown per quarter. On the ri ght, the data
are aggregated to provide the annual sales. NOTE TO EDITOR: P lease update
ﬁgure by replacing years 2002, 2003, 2004 with 2008, 2009, 20 10, respectively.
Thanks.
cuboid. For the sales data of Figure 3.11, the apex cuboid would give o ne total—
the total salesfor all three years, for all item types, and for all branches. Data
cubes created for varying levels of abstraction are often re ferred to as cuboids ,
so that a data cube may instead refer to a lattice of cuboids . Each higher level
of abstraction further reduces the resulting data size. Whe n replying to data
mining requests, the smallest available cuboid relevant to the given task should
be used. This issue is also addressed in Chapter 4.
3.5 Data Transformation and Data Discretiza-
tion
This section presents methods of data transformation. In th is preprocessing
step, the data are transformed or consolidated so that the re sulting mining pro-
cess may be more eﬃcient, and the patterns found may be easier to understand.
Data discretization, a form of data transformation, is also discussed.
3.5.1 Overview of Data Transformation Strategies
Indata transformation , the data are transformed or consolidated into forms
appropriate for mining. Strategies for data transformatio n include the following:
1.Smoothing , which works to remove noise from the data. Such techniques
include binning, regression, and clustering.
2.Attribute construction (orfeature construction ), where new attributes
are constructed and added from the given set of attributes to help the
mining process.
3.5. DATA TRANSFORMATION AND DATA DISCRETIZATION 33
568ABCD
750
150
50home
entertainment
computer
phone
security
2002 2003
yearitem typebranch
2004
Figure 3.11: A data cube for sales at AllElectronics . NOTE TO EDITOR:
Please update ﬁgure by replacing years 2002, 2003, 2004 with 2008, 2009, 2010,
respectively. Thanks.
3.Aggregation , where summary or aggregation operations are applied to
the data. For example, the daily sales data may be aggregated so as to
compute monthly and annual total amounts. This step is typic ally used
in constructing a data cube for analysis of the data at multip le levels of
abstraction.
4.Normalization , where the attribute data are scaled so as to fall within
a smaller range, such as −1.0 to 1 .0, or 0 .0 to 1 .0.
5.Discretization , where the raw values of a numeric attribute (such as age)
are replaced by interval labels (e.g., 0-10, 11-20, and so on ) or conceptual
labels (e.g., youth, adult , andsenior ). The labels, in turn, can be recur-
sively organized into higher-level concepts, resulting in aconcept hierarchy
($600...$800] ($800...$1000] ($400...$600] ($200...$400] ($0...$200]($0...$1000]
($900...
$1000]($800...
$900]($700...
$800]($600...
$700]($500...
$600]($100...
$200]($400...
$500]($0...
$100]($200...
$300]($300...
$400]
Figure 3.12: A concept hierarchy for the attribute price, where an interval
($X . . .$Y] denotes the range from $ X(exclusive) to $ Y(inclusive).
34 CHAPTER 3. DATA PREPROCESSING
for the numeric attribute. Figure 3.12 shows a concept hiera rchy for the
attribute price. More than one concept hierarchy can be deﬁned for the
same attribute in order to accommodate the needs of various u sers.
6.Concept hierarchy generation for nominal data , where attributes
such as street can be generalized to higher-level concepts, like cityor
country . Many hierarchies for nominal attributes are implicit with in the
database schema and can be automatically deﬁned at the schem a deﬁnition
level.
Recall that there is much overlap between the major data prep rocessing
tasks. The ﬁrst three of the above strategies were discussed earlier in this chap-
ter. Smoothing is a form of data cleaning and was addressed in Section 3.2.2.
Section 3.2.3 on the data cleaning process also discussed ET L tools, where users
specify transformations to correct data inconsistencies. Attribute construction
and aggregation were discussed in Section 3.4 on data reduct ion. In this section,
we therefore concentrate on the latter three strategies.
Discretization techniques can be categorized based on how t he discretiza-
tion is performed, such as whether it uses class information or which direction
it proceeds (i.e., top-down vs. bottom-up). If the discreti zation process uses
class information, then we say it is supervised discretization . Otherwise, it is
unsupervised . If the process starts by ﬁrst ﬁnding one or a few points (call ed
split points orcut points ) to split the entire attribute range, and then repeats
this recursively on the resulting intervals, it is called top-down discretization or
splitting . This contrasts with bottom-up discretization ormerging , which starts
by considering all of the continuous values as potential spl it-points, removes
some by merging neighborhood values to form intervals, and t hen recursively
applies this process to the resulting intervals.
Data discretization and concept hierarchy generation are a lso forms of data
reduction. The raw data are replaced by a smaller number of in terval or concept
labels. This simpliﬁes the original data and makes the minin g more eﬃcient.
The resulting patterns mined are typically easier to unders tand. Concept hier-
archies are also useful for mining at multiple levels of abst raction.
The rest of this section is organized as follows. First, norm alization tech-
niques are presented in Section 3.5.2. We then describe seve ral techniques for
data discretization, each of which can be used to generate co ncept hierarchies for
numeric attributes. The techniques include binning (Section 3.5.3), histogram
analysis (Section 3.5.4), as well as cluster analysis ,decision-tree analysis , and
correlation analysis (Section 3.5.5). Finally, Section 3.5.6 describes the auto -
matic generation of concept hierarchies for nominal data.
3.5.2 Data Transformation by Normalization
The measurement unit used can aﬀect the data analysis. For ex ample, changing
measurement units from meters to inches for height , or from kilograms to pounds
forweight , may lead to very diﬀerent results. In general, expressing a n attribute
3.5. DATA TRANSFORMATION AND DATA DISCRETIZATION 35
in smaller units will lead to a larger range for that attribut e, and thus tend to
give such an attribute greater eﬀect or “weight”. To help avo id dependence on
the choice of measurement units, the data should be normalized orstandardized .
This involves transforming the data to fall within a smaller or common range,
such as [ −1,1] or [0.0,1.0]. (The terms “standardize” and “normalize” a re used
interchangeably in data preprocessing, although in statis tics, the latter term
also has other connotations.)
Normalizing the data attempts to give all attributes an equa l weight. Nor-
malization is particularly useful for classiﬁcation algor ithms involving neural
networks, or distance measurements such as nearest-neighb or classiﬁcation and
clustering. If using the neural network backpropagation al gorithm for classi-
ﬁcation mining (Chapter 8), normalizing the input values fo r each attribute
measured in the training tuples will help speed up the learni ng phase. For
distance-based methods, normalization helps prevent attr ibutes with initially
large ranges (e.g., income ) from outweighing attributes with initially smaller
ranges (e.g., binary attributes). It is also useful when giv en no prior knowledge
of the data.
There are many methods for data normalization. We study min-max normal-
ization, z-score normalization, andnormalization by decimal scaling. For our
discussion, let Abe a numeric attribute with nobserved values, v1, v2, . . . , v n.
Min-max normalization performs a linear transformation on the original
data. Suppose that min Aandmax Aare the minimum and maximum values
of an attribute, A. Min-max normalization maps a value, vi, ofAtov′
iin the
range [ newmin A, new max A] by computing
v′
i=vi−min A
max A−min A(newmax A−newmin A) +newmin A. (3.8)
Min-max normalization preserves the relationships among t he original data
values. It will encounter an “out-of-bounds” error if a futu re input case for
normalization falls outside of the original data range for A.
Example 3.4 Min-max normalization. Suppose that the minimum and maximum values
for the attribute income are $12,000 and $98,000, respectively. We would like
to map income to the range [0 .0,1.0]. By min-max normalization, a value of
$73,600 for income is transformed to73,600−12,000
98,000−12,000(1.0−0) + 0 = 0 .716.
Inz-score normalization (orzero-mean normalization ), the values for an
attribute, A, are normalized based on the mean (i.e., average) and standa rd
deviation of A. A value, vi, ofAis normalized to v′
iby computing
v′
i=vi−¯A
σA, (3.9)
where ¯AandσAare the mean and standard deviation, respectively, of attri bute
A. The mean and standard deviation were discussed in Section 2 .2 of this book,
where ¯A=1
n(v1+v2+· · ·+vn) and σAis computed as the square root of the
36 CHAPTER 3. DATA PREPROCESSING
variance of A(see Equation (2.6)). This method of normalization is usefu l when
the actual minimum and maximum of attribute Aare unknown, or when there
are outliers that dominate the min-max normalization.
Example 3.5 z-score Normalization Suppose that the mean and standard deviation of the
values for the attribute income are $54,000 and $16,000, respectively. With z-
score normalization, a value of $73,600 for income is transformed to73,600−54,000
16,000=
1.225.
A variation of the above z-score normalization replaces the standard devia-
tion of Equation 3.9 by the mean absolute deviation ofA. The mean absolute
deviation ofA, denoted sA, is:
sA=1
n(|v1−¯A|+|v2−¯A|+· · ·+|vn−¯A|). (3.10)
Thus, z-score normalization using the mean absolute deviat ion is:
v′
i=vi−¯A
sA. (3.11)
The mean absolute deviation, sA, is more robust to outliers than the standard
deviation, σA. When computing the mean absolute deviation, the deviation s
from the mean (i.e., |xi−¯x|) are not squared; hence, the eﬀect of outliers is
somewhat reduced.
Normalization by decimal scaling normalizes by moving the decimal
point of values of attribute A. The number of decimal points moved depends
on the maximum absolute value of A. A value, vi, ofAis normalized to v′
iby
computing
v′
i=vi
10j, (3.12)
where jis the smallest integer such that Max(|v′
i|)<1.
Example 3.6 Decimal scaling. Suppose that the recorded values of Arange from −986 to
917. The maximum absolute value of Ais 986. To normalize by decimal scaling,
we therefore divide each value by 1,000 (i.e., j= 3) so that −986 normalizes to
−0.986 and 917 normalizes to 0 .917.
Note that normalization can change the original data quite a bit, especially
when using z-score normalization or decimal scaling. It is a lso necessary to
save the normalization parameters (such as the mean and stan dard deviation if
using z-score normalization) so that future data can be norm alized in a uniform
manner.
3.5.3 Discretization by Binning
Binning is a top-down splitting technique based on a speciﬁe d number of bins.
Section 3.2.2 discussed binning methods for data smoothing . These methods
3.5. DATA TRANSFORMATION AND DATA DISCRETIZATION 37
are also used as discretization methods for data reduction a nd concept hierar-
chy generation. For example, attribute values can be discre tized by applying
equal-width or equal-frequency binning, and then replacin g each bin value by
the bin mean or median, as in smoothing by bin means orsmoothing by bin me-
dians, respectively. These techniques can be applied recursivel y to the resulting
partitions in order to generate concept hierarchies.
Binning does not use class information and is therefore an un supervised
discretization technique. It is sensitive to the user-spec iﬁed number of bins, as
well as the presence of outliers.
3.5.4 Discretization by Histogram Analysis
Like binning, histogram analysis is an unsupervised discre tization technique
because it does not use class information. Histograms were i ntroduced in Sec-
tion 2.2.9. A histogram partitions the values of an attribut e,A, into disjoint
ranges called buckets .
Various partitioning rules can be used to deﬁne histograms ( Section 3.4.6).
In anequal-width histogram, for example, the values are partitioned into equ al-
sized partitions or ranges (such as in Figure 3.8 for price, where each bucket has
a width of $10). With an equal-frequency histogram, the values are partitioned
so that, ideally, each partition contains the same number of data tuples. The
histogram analysis algorithm can be applied recursively to each partition in or-
der to automatically generate a multilevel concept hierarc hy, with the procedure
terminating once a prespeciﬁed number of concept levels has been reached. A
minimum interval size can also be used per level to control the recursive proce-
dure. This speciﬁes the minimum width of a partition, or the m inimum number
of values for each partition at each level. Histograms can al so be partitioned
based on cluster analysis of the data distribution, as descr ibed below.
3.5.5 Discretization by Cluster, Decision Tree, and Cor-
relation Analyses
Clustering, decision tree analysis, and correlation analy sis can be used for data
discretization. We brieﬂy study each of these approaches.
Cluster analysis is a popular data discretization method. A clustering algo-
rithm can be applied to discretize a numeric attribute, A, by partitioning the
values of Ainto clusters or groups. Clustering takes the distribution ofAinto
consideration, as well as the closeness of data points, and t herefore is able to
produce high-quality discretization results.
Clustering can be used to generate a concept hierarchy for Aby following
either a top-down splitting strategy or a bottom-up merging strategy, where
each cluster forms a node of the concept hierarchy. In the for mer, each initial
cluster or partition may be further decomposed into several subclusters, forming
a lower level of the hierarchy. In the latter, clusters are fo rmed by repeatedly
grouping neighboring clusters in order to form higher-leve l concepts. Clustering
methods for data mining are studied in Chapters 10 and 11.
38 CHAPTER 3. DATA PREPROCESSING
Techniques to generate decision trees for classiﬁcation (C hapter 8) can be ap-
plied to discretization. Such techniques employ a top-down splitting approach.
Unlike the other methods mentioned so far, decision tree app roaches to dis-
cretization are supervised, that is, they make use of class l abel information. For
example, we may have a data set of patient symptoms (the attri butes), where
each patient has an associated diagnosis class label. Class distribution infor-
mation is used in the calculation and determination of split -points (data values
for partitioning an attribute range). Intuitively, the mai n idea is to select split-
points so that a given resulting partition contains as many t uples of the same
class as possible. Entropy is the most commonly used measure for this purpose.
To discretize a numeric attribute, A, the method selects the value of Athat has
the minimum entropy as a split-point, and recursively parti tions the resulting
intervals to arrive at a hierarchical discretization. Such discretization forms a
concept hierarchy for A.
Because decision-tree-based discretization uses class in formation, it is more
likely that the interval boundaries (split-points) are deﬁ ned to occur in places
that may help improve classiﬁcation accuracy. Decision tre es and the entropy
measure are described in greater detail in Section 8.3.2.
Measures of correlation can be used for discretization. ChiMerge is aχ2-
based discretization method. The discretization methods t hat we have studied
up to this point have all employed a top-down, splitting stra tegy. This contrasts
with ChiMerge, which employs a bottom-up approach by ﬁnding the best neigh-
boring intervals and then merging these to form larger inter vals, recursively. As
with decision tree analysis, ChiMerge is supervised in that it uses class infor-
mation. The basic notion is that for accurate discretizatio n, the relative class
frequencies should be fairly consistent within an interval . Therefore, if two ad-
jacent intervals have a very similar distribution of classe s, then the intervals can
be merged. Otherwise, they should remain separate.
ChiMerge proceeds as follows. Initially, each distinct val ue of a numeric
attribute Ais considered to be one interval. χ2tests are performed for every
pair of adjacent intervals. Adjacent intervals with the lea stχ2values are merged
together, because low χ2values for a pair indicate similar class distributions.
This merging process proceeds recursively until a predeﬁne d stopping criterion
is met.
3.5.6 Concept Hierarchy Generation for Nominal Data
We now look at data transformation for nominal data. In parti cular, we study
the generation of concept hierarchies for nominal attribut es. Nominal attributes
have a ﬁnite (but possibly large) number of distinct values, with no ordering
among the values. Examples include geographic location ,job category , anditem
type.
Manual deﬁnition of concept hierarchies can be a tedious and time-consuming
task for a user or a domain expert. Fortunately, many hierarc hies are implicit
within the database schema and can be automatically deﬁned a t the schema
deﬁnition level. The concept hierarchies can be used to tran sform the data into
3.5. DATA TRANSFORMATION AND DATA DISCRETIZATION 39
multiple levels of granularity. For example, data mining pa tterns regarding sales
may be found relating to speciﬁc regions or countries, in add ition to individual
branch locations.
We study four methods for the generation of concept hierarch ies for nominal
data.
1. Speciﬁcation of a partial ordering of attributes explici tly at the
schema level by users or experts: Concept hierarchies for nominal
attributes or dimensions typically involve a group of attri butes. A user
or an expert can easily deﬁne a concept hierarchy by specifyi ng a partial
or total ordering of the attributes at the schema level. For e xample, sup-
pose that a relational database contains the following grou p of attributes:
street, city, province orstate, andcountry . Similarly, a location dimension
of a data warehouse may contain the same attributes. A hierar chy can
be deﬁned by specifying the total ordering among these attri butes at the
schema level, such as street <city<province orstate<country .
2. Speciﬁcation of a portion of a hierarchy by explicit data g rouping:
This is essentially the manual deﬁnition of a portion of a con cept hier-
archy. In a large database, it is unrealistic to deﬁne an enti re concept
hierarchy by explicit value enumeration. On the contrary, w e can easily
specify explicit groupings for a small portion of intermedi ate-level data.
For example, after specifying that province andcountry form a hierarchy
at the schema level, a user could deﬁne some intermediate lev els man-
ually, such as “ {Alberta, Saskatchewan, Manitoba } ⊂prairies Canada ”
and “ {British Columbia, prairies Canada } ⊂Western Canada ”.
3. Speciﬁcation of a set of attributes , but not of their partial order-
ing:A user may specify a set of attributes forming a concept hiera rchy,
but omit to explicitly state their partial ordering. The sys tem can then
try to automatically generate the attribute ordering so as t o construct a
meaningful concept hierarchy.
“Without knowledge of data semantics, how can a hierarchica l ordering for
an arbitrary set of nominal attributes be found?” Consider the following
observation that since higher-level concepts generally co ver several subor-
dinate lower-level concepts, an attribute deﬁning a high co ncept level (e.g.,
country ) will usually contain a smaller number of distinct values th an an
attribute deﬁning a lower concept level (e.g., street). Based on this obser-
vation, a concept hierarchy can be automatically generated based on the
number of distinct values per attribute in the given attribu te set. The
attribute with the most distinct values is placed at the lowe st level of
the hierarchy. The lower the number of distinct values an att ribute has,
the higher it is in the generated concept hierarchy. This heu ristic rule
works well in many cases. Some local-level swapping or adjus tments may
be applied by users or experts, when necessary, after examin ation of the
generated hierarchy.
40 CHAPTER 3. DATA PREPROCESSING
country 15 distinct values
province_or_state
city
street365 distinct values
3,567 distinct values
674,339 distinct values
Figure 3.13: Automatic generation of a schema concept hiera rchy based on the
number of distinct attribute values.
Let’s examine an example of this third method.
Example 3.7 Concept hierarchy generation based on the number of distinc t values
per attribute. Suppose a user selects a set of location-oriented attribute s,
street, country, province orstate, and city, from the AllElectronics database,
but does not specify the hierarchical ordering among the att ributes.
A concept hierarchy for location can be generated automatically, as illus-
trated in Figure 3.13. First, sort the attributes in ascendi ng order based on
the number of distinct values in each attribute. This result s in the following
(where the number of distinct values per attribute is shown i n parentheses):
country (15),province orstate (365), city(3567), and street (674,339). Sec-
ond, generate the hierarchy from the top down according to th e sorted order,
with the ﬁrst attribute at the top level and the last attribut e at the bottom
level. Finally, the user can examine the generated hierarch y, and when neces-
sary, modify it to reﬂect desired semantic relationships am ong the attributes.
In this example, it is obvious that there is no need to modify t he generated
hierarchy.
Note that this heuristic rule is not foolproof. For example, a time dimension
in a database may contain 20 distinct years, 12 distinct mont hs, and 7 distinct
days of the week. However, this does not suggest that the time hierarchy should
be “year<month <daysoftheweek”, with daysoftheweekat the top of the
hierarchy.
4. Speciﬁcation of only a partial set of attributes: Sometimes a user can
be careless when deﬁning a hierarchy, or have only a vague ide a about
3.6. SUMMARY 41
what should be included in a hierarchy. Consequently, the us er may have
included only a small subset of the relevant attributes in th e hierarchy
speciﬁcation. For example, instead of including all of the h ierarchically
relevant attributes for location , the user may have speciﬁed only street
andcity. To handle such partially speciﬁed hierarchies, it is impor tant
to embed data semantics in the database schema so that attrib utes with
tight semantic connections can be pinned together. In this w ay, the speci-
ﬁcation of one attribute may trigger a whole group of semanti cally tightly
linked attributes to be “dragged in” to form a complete hiera rchy. Users,
however, should have the option to override this feature, as necessary.
Example 3.8 Concept hierarchy generation using prespeciﬁed semantic c onnections.
Suppose that a data mining expert (serving as an administrat or) has pinned
together the ﬁve attributes number, street, city, province orstate, andcountry ,
because they are closely linked semantically regarding the notion of location . If
a user were to specify only the attribute cityfor a hierarchy deﬁning location ,
the system can automatically drag in all of the above ﬁve sema ntically related
attributes to form a hierarchy. The user may choose to drop an y of these
attributes, such as number andstreet, from the hierarchy, keeping cityas the
lowest conceptual level in the hierarchy.
In summary, information at the schema level and of attribute -value counts
can be used to generate concept hierarchies for nominal data . Transforming
nominal data with the use of concept hierarchies allows high er-level knowledge
patterns to be found. It allows mining at multiple levels of a bstraction, which
is a common requirement for data mining applications.
3.6 Summary
•Data quality is deﬁned in terms of accuracy, completeness, consistency,
timeliness, believability , and interpretabilty . These qualities are assessed
based on the intended use of the data.
•Data cleaning routines attempt to ﬁll in missing values, smooth out noise
while identifying outliers, and correct inconsistencies i n the data. Data
cleaning is usually performed as an iterative two-step proc ess consisting
of discrepancy detection and data transformation.
•Data integration combines data from multiple sources to form a co-
herent data store. The resolution of semantic heterogeneit y, metadata,
correlation analysis, tuple duplication detection, and da ta conﬂict detec-
tion contribute toward smooth data integration.
•Data reduction techniques obtain a reduced representation of the data
while minimizing the loss of information content. These inc lude methods
ofdimensionality reduction ,numerosity reduction , anddata compression .
Dimensionality reduction reduces the number of random variables or
42 CHAPTER 3. DATA PREPROCESSING
attributes under consideration. Methods include wavelet transforms, prin-
cipal components analysis, attribute subset selection , and attribute cre-
ation.Numerosity reduction methods use parametric or nonparatmet-
ric models to obtain smaller representations of the origina l data. Para-
metric models store only the model parameters instead of the actual data.
Examples include regression and log-linear models. Nonpar amteric meth-
ods include histograms, clustering, sampling, and data cub e aggregation.
Data compression methods apply transformations to obtain a reduced
or “compressed” representation of the original data. The da ta reduction
islossless if the original data can be reconstructed from the compresse d
data without any loss of information; otherwise, it is lossy.
•Data transformation routines convert the data into appropriate forms
for mining. For example, in normalization , attribute data are scaled so
as to fall within a small range such as 0 .0 to 1.0. Other examples are data
discretization andconcept hierarchy generation .
•Data discretization transforms numeric data by mapping values to in-
terval or concept labels. Such methods can be used to automat ically gen-
erateconcept hierarchies for the data, which allows for mining at multiple
levels of granularity. Discretization techniques include binning, histogram
analysis, cluster analysis, decision-tree analysis, and c orrelation analy-
sis. For nominal data, concept hierarchies may be generated based on
schema deﬁnitions as well as the number of distinct values pe r attribute.
•Although numerous methods of data preprocessing have been d eveloped,
data preprocessing remains an active area of research, due t o the huge
amount of inconsistent or dirty data and the complexity of th e problem.
3.7 Exercises
1.Data quality can be assessed in terms of several issues, including accura cy,
completeness, and consistency. For each of the above three i ssues, discuss
how the assessment of data quality can depend on the intended use of the
data, giving examples Propose two other dimensions of data q uality.
2. In real-world data, tuples with missing values for some attributes are a
common occurrence. Describe various methods for handling t his problem.
3. Exercise 2.2 gave the following data (in increasing order ) for the attribute
age: 13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33 , 35,
35, 35, 35, 36, 40, 45, 46, 52, 70.
(a) Use smoothing by bin means to smooth the above data, using a bin
depth of 3. Illustrate your steps. Comment on the eﬀect of thi s
technique for the given data.
(b) How might you determine outliers in the data?
3.7. EXERCISES 43
(c) What other methods are there for data smoothing ?
4. Discuss issues to consider during data integration .
5. What are the value ranges of the following normalization methods ?
(a) min-max normalization
(b) z-score normalization
(c) z-score normalization using the mean absolute deviatio n instead of
standard deviation
(d) normalization by decimal scaling
6. Use the methods below to normalize the following group of data:
200,300,400,600,1000
(a) min-max normalization by setting min= 0 and max= 1
(b) z-score normalization
(c) z-score normalization using the mean absolute deviatio n instead of
standard deviation
(d) normalization by decimal scaling
7. Using the data for agegiven in Exercise 3.3, answer the following:
(a) Use min-max normalization to transform the value 35 for ageonto
the range [0 .0,1.0].
(b) Use z-score normalization to transform the value 35 for age, where
the standard deviation of ageis 12.94 years.
(c) Use normalization by decimal scaling to transform the va lue 35 for
age.
(d) Comment on which method you would prefer to use for the giv en
data, giving reasons as to why.
8. Using the data for ageandbody fat given in Exercise 2.4, answer the
following:
(a) Normalize the two attributes based on z-score normalization .
(b) Calculate the correlation coeﬃcient (Pearson’s product moment coef-
ﬁcient). Are these two attributes positively or negatively correlated?
Compute their covariance.
9. Suppose a group of 12 sales price records has been sorted as follows:
5,10,11,13,15,35,50,55,72,92,204,215.
Partition them into three bins by each of the following metho ds.
44 CHAPTER 3. DATA PREPROCESSING
(a) equal-frequency (equidepth) partitioning
(b) equal-width partitioning
(c) clustering
10. Use a ﬂowchart to summarize the following procedures for attribute subset
selection :
(a) stepwise forward selection
(b) stepwise backward elimination
(c) a combination of forward selection and backward elimina tion
11. Using the data for agegiven in Exercise 3.3,
(a) Plot an equal-width histogram of width 10.
(b) Sketch examples of each of the following sampling techni ques: SR-
SWOR, SRSWR, cluster sampling, stratiﬁed sampling. Use sam ples
of size 5 and the strata “youth”, “middle-aged”, and “senior ”.
12. ChiMerge [Ker92] is a supervised, bottom-up (i.e., merg e-based) data dis-
cretization method. It relies on χ2analysis: adjacent intervals with the
leastχ2values are merged together till the chosen stopping criteri on sat-
isﬁes.
(a) Brieﬂy describe how ChiMerge works.
(b) Take the IRIS data set, obtained from the UC-Irvine Machi ne Learn-
ing Data Repository ( http://www.ics.uci.edu/ ∼mlearn/MLRepository.html ),
as a data set to be discretized. Perform data discretization for each
of the four numerical attributes using the ChiMerge method. (Let
the stopping criteria be: max-interval = 6). You need to write a
small program to do this to avoid clumsy numerical computati on.
Submit your simple analysis and your test results: split poi nts, ﬁnal
intervals, and your documented source program.
13. Propose an algorithm, in pseudocode or in your favorite p rogramming
language, for the following:
(a) The automatic generation of a concept hierarchy for cate gorical data
based on the number of distinct values of attributes in the gi ven
schema
(b) The automatic generation of a concept hierarchy for nume rical data
based on the equal-width partitioning rule
(c) The automatic generation of a concept hierarchy for nume rical data
based on the equal-frequency partitioning rule
3.8. BIBLIOGRAPHIC NOTES 45
14. Robust data loading poses a challenge in database system s because the in-
put data are often dirty. In many cases, an input record may mi ss multiple
values, some records could be contaminated , with some data values out of
range or of a diﬀerent data type than expected. Work out an aut omated
data cleaning and loading algorithm so that the erroneous data will be
marked, and contaminated data will not be mistakenly insert ed into the
database during data loading.
3.8 Bibliographic Notes
Data preprocessing is discussed in a number of textbooks, in cluding English
[Eng99], Pyle [Pyl99], Loshin [Los01], Redman [Red01], and Dasu and Johnson
[DJ03]. More speciﬁc references to individual preprocessi ng techniques are given
below.
For discussion regarding data quality, see Redman [Red92], Wang, Storey,
and Firth [WSF95], Wand and Wang [WW96], Ballou and Tayi [BT9 9], and
Olson [Ols03]. Potter’s Wheel ( control.cx.berkely.edu/abc ), the interac-
tive data cleaning tool described in Section 3.2.3, is prese nted in Raman and
Hellerstein [RH01]. An example of the development of declar ative languages for
the speciﬁcation of data transformation operators is given in Galhardas et al.
[GFS+01]. The handling of missing attribute values is discussed i n Friedman
[Fri77], Breiman, Friedman, Olshen, and Stone [BFOS84], an d Quinlan [Qui89].
Hua and Pei [HP07] presented a heuristic approach to clean disguised miss-
ing data , where such data is captured when users falsely select defau lt values
on forms (such as ‘January 1’ for birthdate ) when they do not want to dis-
close personal information. A method for the detection of ou tlier or “garbage”
patterns in a handwritten character database is given in Guy on, Matic, and
Vapnik [GMV96]. Binning and data normalization are treated in many texts,
including [KLV+98], [WI98], [Pyl99]. Systems that include attribute (or fe a-
ture) construction include BACON by Langley, Simon, Bradsh aw, and Zytkow
[LSBZ87], Stagger by Schlimmer [Sch86], FRINGE by Pagallo [ Pag89], and
AQ17-DCI by Bloedorn and Michalski [BM98]. Attribute const ruction is also
described in Liu and Motoda [LM98, Le98]. Dasu, et al. built a BELLMAN
system and proposed a set of interesting methods for buildin g a data quality
browser by mining database structures [DJMS02].
A good survey of data reduction techniques can be found in Bar bar´ a et
al. [BDF+97]. For algorithms on data cubes and their precomputation, see
[SS94, AAD+96, HRU96, RS97, ZDN97]. Attribute subset selection (or feature
subset selection ) is described in many texts, such as Neter, Kutner, Nacht-
sheim, and Wasserman [NKNW96], Dash and Liu [DL97], and Liu a nd Motoda
[LM98, LM98b]. A combination forward selection and backwar d elimination
method was proposed in Siedlecki and Sklansky [SS88]. A wrap per approach
to attribute selection is described in Kohavi and John [KJ97 ]. Unsupervised
attribute subset selection is described in Dash, Liu, and Ya o [DLY97]. For
a description of wavelets for dimensionality reduction, se e Press, Teukolosky,
46 CHAPTER 3. DATA PREPROCESSING
Vetterling, and Flannery [PTVF96]. A general account of wav elets can be
found in Hubbard [Hub96]. For a list of wavelet software pack ages, see Bruce,
Donoho, and Gao [BDG96]. Daubechies transforms are describ ed in Daubechies
[Dau92]. The book by Press, et al. [PTVF96] includes an intro duction to
singular value decomposition for principal components ana lysis. Routines for
PCA are included in most statistical software packages, suc h as SAS ( http:
//www.sas.com/SASHome.html ).
An introduction to regression and log-linear models can be f ound in several
textbooks, such as [Jam85, Dob90, JW92, Dev95, NKNW96]. For log-linear
models (known as multiplicative models in the computer science literature),
see Pearl [Pea88]. For a general introduction to histograms , see Barbar´ a et
al. [BDF+97] and Devore and Peck [DP97]. For extensions of single attr ibute
histograms to multiple attributes, see Muralikrishna and D eWitt [MD88] and
Poosala and Ioannidis [PI97]. Several references to cluste ring algorithms are
given in Chapter 7 of this book, which is devoted to the topic. A survey of
multidimensional indexing structures is given in Gaede and G¨ unther [GG98].
The use of multidimensional index trees for data aggregatio n is discussed in Aoki
[Aok98]. Index trees include R-trees (Guttman [Gut84]), qu ad-trees (Finkel
and Bentley [FB74]), and their variations. For discussion o n sampling and data
mining, see Kivinen and Mannila [KM94] and John and Langley [ JL96].
There are many methods for assessing attribute relevance. E ach has its own
bias. The information gain measure is biased towards attrib utes with many
values. Many alternatives have been proposed, such as gain r atio (Quinlan
[Qui93]), which considers the probability of each attribut e value. Other rele-
vance measures include the gini index (Breiman, Friedman, O lshen, and Stone
[BFOS84]), the χ2contingency table statistic, and the uncertainty coeﬃcien t
(Johnson and Wichern [JW92]). For a comparison of attribute selection mea-
sures for decision tree induction, see Buntine and Niblett [ BN92]. For additional
methods, see Liu and Motoda [LM98]b, Dash and Liu [DL97], and Almuallim
and Dietterich [AD91].
Liu et al. [LHTD02] performed a comprehensive survey of data discretiza-
tion methods. Entropy-based discretization with the C4.5 a lgorithm is described
in Quinlan [Qui93]. In Catlett [Cat91], the D-2 system binar izes a numerical
feature recursively. ChiMerge by Kerber [Ker92] and Chi2 by Liu and Setiono
[LS95] are methods for the automatic discretization of nume rical attributes that
both employ the χ2statistic. Fayyad and Irani [FI93] apply the minimum de-
scription length principle to determine the number of inter vals for numerical
discretization. Concept hierarchies and their automatic g eneration from cate-
gorical data are described in Han and Fu [HF94].
Bibliography
[AAD+96] S. Agarwal, R. Agrawal, P. M. Deshpande, A. Gupta, J. F.
Naughton, R. Ramakrishnan, and S. Sarawagi. On the computa-
tion of multidimensional aggregates. In Proc. 1996 Int. Conf. Very
Large Data Bases (VLDB’96) , pages 506–521, Bombay, India, Sept.
1996.
[AD91] H. Almuallim and T. G. Dietterich. Learning with many irrelevant
features. In Proc. 1991 Nat. Conf. Artiﬁcial Intelligence (AAAI’91) ,
pages 547–552, Anaheim, CA, July 1991.
[Aok98] P. M. Aoki. Generalizing “search” in generalized se arch trees. In
Proc. 1998 Int. Conf. Data Engineering (ICDE’98) , pages 380–389,
Orlando, FL, Feb. 1998.
[BDF+97] D. Barbar´ a, W. DuMouchel, C. Faloutos, P. J. Haas, J. H. H eller-
stein, Y. Ioannidis, H. V. Jagadish, T. Johnson, R. Ng, V. Poo sala,
K. A. Ross, and K. C. Servcik. The New Jersey data reduction
report. Bull. Technical Committee on Data Engineering , 20:3–45,
Dec. 1997.
[BDG96] A. Bruce, D. Donoho, and H.-Y. Gao. Wavelet analysis . InIEEE
Spectrum , pages 26–35, Oct 1996.
[BFOS84] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classiﬁcation
and Regression Trees . Wadsworth International Group, 1984.
[BM98] A. Blum and T. Mitchell. Combining labeled and unlabe led data
with co-training. In Proc. 11th Conf. on Computational Learning
Theory (COLT’ 98) , pages 92–100, Madison, WI, 1998.
[BN92] W. L. Buntine and T. Niblett. A further comparison of s plitting
rules for decision-tree induction. Machine Learning , 8:75–85, 1992.
[BT99] D. P. Ballou and G. K. Tayi. Enhancing data quality in d ata ware-
house environments. Comm. ACM , 42:73–78, 1999.
[Cat91] J. Catlett. Megainduction: Machine Learning on Very large
Databases . Ph.D. Thesis, University of Sydney, 1991.
47
48 BIBLIOGRAPHY
[Dau92] I. Daubechies. Ten Lectures on Wavelets . Capital City Press, 1992.
[Dev95] J. L. Devore. Probability and Statistics for Engineering and the
Science (4th ed.). Duxbury Press, 1995.
[DJ03] T. Dasu and T. Johnson. Exploratory Data Mining and Data Clean-
ing. John Wiley & Sons, 2003.
[DJMS02] T. Dasu, T. Johnson, S. Muthukrishnan, and V. Shkap enyuk. Min-
ing database structure; or how to build a data quality browse r.
InProc. 2002 ACM-SIGMOD Int. Conf. on Management of Data
(SIGMOD’02) , pages 240–251, Madison, WI, June 2002.
[DL97] M. Dash and H. Liu. Feature selection methods for clas siﬁcation.
Intelligent Data Analysis , 1:131–156, 1997.
[DLY97] M. Dash, H. Liu, and J. Yao. Dimensionality reductio n of unsu-
pervised data. In Proc. 1997 IEEE Int. Conf. Tools with AI (IC-
TAI’97) , pages 532–539, IEEE Computer Society, 1997.
[Dob90] A. J. Dobson. An Introduction to Generalized Linear Models . Chap-
man and Hall, 1990.
[DP97] J. Devore and R. Peck. Statistics: The Exploration and Analysis of
Data. Duxbury Press, 1997.
[Eng99] L. English. Improving Data Warehouse and Business Information
Quality: Methods for Reducing Costs and Increasing Proﬁts . John
Wiley & Sons, 1999.
[FB74] R. A. Finkel and J. L. Bentley. Quad-trees: A data stru cture for
retrieval on composite keys. ACTA Informatica , 4:1–9, 1974.
[FI93] U. Fayyad and K. Irani. Multi-interval discretizati on of continuous-
values attributes for classiﬁcation learning. In Proc. 1993 Int. Joint
Conf. Artiﬁcial Intelligence (IJCAI’93) , pages 1022–1029, Cham-
bery, France, 1993.
[Fri77] J. H. Friedman. A recursive partitioning decision r ule for nonpara-
metric classiﬁers. IEEE Trans. Computer , 26:404–408, 1977.
[GFS+01] H. Galhardas, D. Florescu, D. Shasha, E. Simon, and C.-A. Saita.
Declarative data cleaning: Language, model, and algorithm s. In
Proc. 2001 Int. Conf. on Very Large Data Bases (VLDB’01) , pages
371–380, Rome, Italy, Sept. 2001.
[GG98] V. Gaede and O. G¨ unther. Multidimensional access me thods. ACM
Comput. Surv. , 30:170–231, 1998.
BIBLIOGRAPHY 49
[GMV96] I. Guyon, N. Matic, and V. Vapnik. Discoverying info rmative pat-
terns and data cleaning. In U. M. Fayyad, G. Piatetsky-Shapi ro,
P. Smyth, and R. Uthurusamy, editors, Advances in Knowledge Dis-
covery and Data Mining , pages 181–203. AAAI/MIT Press, 1996.
[Gut84] A. Guttman. R-tree: A dynamic index structure for sp atial search-
ing. In Proc. 1984 ACM-SIGMOD Int. Conf. Management of Data
(SIGMOD’84) , pages 47–57, Boston, MA, June 1984.
[HF94] J. Han and Y. Fu. Dynamic generation and reﬁnement of c on-
cept hierarchies for knowledge discovery in databases. In Proc.
AAAI’94 Workshop Knowledge Discovery in Databases (KDD’94 ),
pages 157–168, Seattle, WA, July 1994.
[HP07] M. Hua and J. Pei. Cleaning disguised missing data: A h euristic
approach. In Proc. 2007 ACM SIGKDD Intl. Conf. on Knowledge
Discovery and Data Mining (KDD’07) , San Jose, CA, Aug. 2007.
[HRU96] V. Harinarayan, A. Rajaraman, and J. D. Ullman. Impl ementing
data cubes eﬃciently. In Proc. 1996 ACM-SIGMOD Int. Conf.
Management of Data (SIGMOD’96) , pages 205–216, Montreal,
Canada, June 1996.
[Hub96] B. B. Hubbard. The World According to Wavelets . A. K. Peters,
1996.
[Jam85] M. James. Classiﬁcation Algorithms . John Wiley & Sons, 1985.
[JL96] G. H. John and P. Langley. Static versus dynamic sampl ing for
data mining. In Proc. 1996 Int. Conf. Knowledge Discovery and
Data Mining (KDD’96) , pages 367–370, Portland, OR, Aug. 1996.
[JW92] R. A. Johnson and D. A. Wichern. Applied Multivariate Statistical
Analysis (3rd ed.). Prentice Hall, 1992.
[Ker92] R. Kerber. Discretization of numeric attributes. I nProc. 1992 Nat.
Conf. Artiﬁcial Intelligence (AAAI’92) , pages 123–128, AAAI/MIT
Press, 1992.
[KJ97] R. Kohavi and G. H. John. Wrappers for feature subset s election.
Artiﬁcial Intelligence , 97:273–324, 1997.
[KLV+98] R. L Kennedy, Y. Lee, B. Van Roy, C. D. Reed, and R. P. Lipp-
man.Solving Data Mining Problems Through Pattern Recognition .
Prentice Hall, 1998.
[KM94] J. Kivinen and H. Mannila. The power of sampling in kno wledge
discovery. In Proc. 13th ACM Symp. Principles of Database Sys-
tems, pages 77–85, Minneapolis, MN, May 1994.
50 BIBLIOGRAPHY
[Le98] H. Liu and H. Motoda (eds.). Feature Extraction, Construction, and
Selection: A Data Mining Perspective . Kluwer Academic, 1998.
[LHTD02] H. Liu, F. Hussain, C. L. Tan, and M. Dash. Discretiz ation: An
enabling technique. Data Mining and Knowledge Discovery , 6:393–
423, 2002.
[LM98] H. Liu and H. Motoda. Feature Selection for Knowledge Discovery
and Data Mining . Kluwer Academic, 1998.
[Los01] D. Loshin. Enterprise Knowledge Management: The Data Quality
Approach . Morgan Kaufmann, 2001.
[LS95] H. Liu and R. Setiono. Chi2: Feature selection and dis cretization
of numeric attributes. In Proc. 1995 IEEE Int. Conf. Tools with AI
(ICTAI’95) , pages 388–391, Washington, DC, Nov. 1995.
[LSBZ87] P. Langley, H. A. Simon, G. L. Bradshaw, and J. M. Zyt kow.Sci-
entiﬁc Discovery: Computational Explorations of the Creat ive Pro-
cesses . MIT Press, 1987.
[MD88] M. Muralikrishna and D. J. DeWitt. Equi-depth histog rams for
extimating selectivity factors for multi-dimensional que ries. In
Proc. 1988 ACM-SIGMOD Int. Conf. Management of Data (SIG-
MOD’88) , pages 28–36, Chicago, IL, June 1988.
[NKNW96] J. Neter, M. H. Kutner, C. J. Nachtsheim, and L. Wass erman.
Applied Linear Statistical Models (4th ed.). Irwin, 1996.
[Ols03] J. E. Olson. Data Quality: The Accuracy Dimension . Morgan
Kaufmann, 2003.
[Pag89] G. Pagallo. Learning DNF by decision trees. In Proc. 1989 Int.
Joint Conf. Artiﬁcial Intelligence (IJCAI’89) , pages 639–644, Mor-
gan Kaufmann, 1989.
[Pea88] J. Pearl. Probabilistic Reasoning in Intelligent Systems . Morgan
Kauﬀman, 1988.
[PI97] V. Poosala and Y. Ioannidis. Selectivity estimation without the
attribute value independence assumption. In Proc. 1997 Int. Conf.
Very Large Data Bases (VLDB’97) , pages 486–495, Athens, Greece,
Aug. 1997.
[PTVF96] W. H. Press, S. A. Teukolosky, W. T. Vetterling, and B. P. Flan-
nery. Numerical Recipes in C: The Art of Scientiﬁc Computing .
Cambridge University Press, 1996.
[Pyl99] D. Pyle. Data Preparation for Data Mining . Morgan Kaufmann,
1999.
BIBLIOGRAPHY 51
[Qui86] J. R. Quinlan. Induction of decision trees. Machine Learning , 1:81–
106, 1986.
[Qui89] J. R. Quinlan. Unknown attribute values in inductio n. InProc. 1989
Int. Conf. Machine Learning (ICML’89) , pages 164–168, Ithaca,
NY, June 1989.
[Qui93] J. R. Quinlan. C4.5: Programs for Machine Learning . Morgan
Kaufmann, 1993.
[Red92] T. Redman. Data Quality: Management and Technology . Bantam
Books, 1992.
[Red01] T. Redman. Data Quality: The Field Guide . Digital Press (Else-
vier), 2001.
[RH01] V. Raman and J. M. Hellerstein. Potter’s wheel: An int eractive
data cleaning system. In Proc. 2001 Int. Conf. on Very Large Data
Bases (VLDB’01) , pages 381–390, Rome, Italy, Sept. 2001.
[RS97] K. Ross and D. Srivastava. Fast computation of sparse datacubes.
InProc. 1997 Int. Conf. Very Large Data Bases (VLDB’97) , pages
116–125, Athens, Greece, Aug. 1997.
[Sch86] J. C. Schlimmer. Learning and representation chang e. In Proc.
1986 Nat. Conf. Artiﬁcial Intelligence (AAAI’86) , pages 511–515,
Philadelphia, PA, 1986.
[SS88] W. Siedlecki and J. Sklansky. On automatic feature se lection. Int.
J. Pattern Recognition and Artiﬁcial Intelligence , 2:197–220, 1988.
[SS94] S. Sarawagi and M. Stonebraker. Eﬃcient organizatio n of large
multidimensional arrays. In Proc. 1994 Int. Conf. Data Engineering
(ICDE’94) , pages 328–336, Houston, TX, Feb. 1994.
[WI98] S. M. Weiss and N. Indurkhya. Predictive Data Mining . Morgan
Kaufmann, 1998.
[WSF95] R. Wang, V. Storey, and C. Firth. A framework for anal ysis of data
quality research. IEEE Trans. Knowledge and Data Engineering ,
7:623–640, 1995.
[WW96] Y. Wand and R. Wang. Anchoring data quality dimension s in on-
tological foundations. Comm. ACM , 39:86–95, 1996.
[ZDN97] Y. Zhao, P. M. Deshpande, and J. F. Naughton. An array -
based algorithm for simultaneous multidimensional aggreg ates. In
Proc. 1997 ACM-SIGMOD Int. Conf. Management of Data (SIG-
MOD’97) , pages 159–170, Tucson, AZ, May 1997.
